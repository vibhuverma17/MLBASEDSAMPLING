{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vibhuverma17/MLBASEDSAMPLING/blob/main/TUANDROMD_Sampling_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aad7i3jF5Rcf",
        "outputId": "90088e16-557f-42fc-9e9e-9aea1b8cba39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting umap-learn\n",
            "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting hdbscan\n",
            "  Downloading hdbscan-0.8.40-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting gower\n",
            "  Downloading gower-0.1.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting kmodes\n",
            "  Downloading kmodes-0.12.2-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: XGBoost in /usr/local/lib/python3.10/dist-packages (2.1.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.5.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.60.0)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.6)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan) (1.4.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from XGBoost) (2.23.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n",
            "Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hdbscan-0.8.40-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gower-0.1.2-py3-none-any.whl (5.2 kB)\n",
            "Downloading kmodes-0.12.2-py2.py3-none-any.whl (20 kB)\n",
            "Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gower, pynndescent, kmodes, hdbscan, umap-learn\n",
            "Successfully installed gower-0.1.2 hdbscan-0.8.40 kmodes-0.12.2 pynndescent-0.5.13 umap-learn-0.5.7\n"
          ]
        }
      ],
      "source": [
        "!pip install umap-learn hdbscan gower kmodes XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ov6EwiM05WKy"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# Standard Libraries\n",
        "# ===============================\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import ast\n",
        "\n",
        "# ===============================\n",
        "# Visualization Libraries\n",
        "# ===============================\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "\n",
        "# ===============================\n",
        "# Scikit-learn Components\n",
        "# ===============================\n",
        "# Data Splitting and Preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Models\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, RandomForestRegressor,\n",
        "    GradientBoostingClassifier, GradientBoostingRegressor,\n",
        "    IsolationForest\n",
        ")\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import (\n",
        "    f1_score, roc_auc_score, accuracy_score, recall_score, precision_score,\n",
        "    mean_absolute_error, mean_squared_error, r2_score,\n",
        "    mean_absolute_percentage_error, roc_curve\n",
        ")\n",
        "\n",
        "# Pairwise distances\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "\n",
        "# ===============================\n",
        "# External Libraries\n",
        "# ===============================\n",
        "import xgboost as xgb  # XGBoost library\n",
        "from scipy.stats import ks_2samp, entropy  # Statistical tests\n",
        "from scipy.spatial.distance import cdist\n",
        "from kmodes.kprototypes import KPrototypes  # Clustering\n",
        "import umap  # Dimensionality reduction\n",
        "import hdbscan  # Density-based clustering\n",
        "import gower  # Gower similarity for mixed data types\n",
        "\n",
        "# ===============================\n",
        "# Configure Warnings\n",
        "# ===============================\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBV8SEqcwY-L"
      },
      "source": [
        "#### DIM REDUCTION AND GRID SEARCH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XQsa-dNvH2vc"
      },
      "outputs": [],
      "source": [
        "class StratifiedSamplingUMAP:\n",
        "    def __init__(self, n_neighbors=15, min_dist=0.1, n_components=2, n_grids=10, sample_percentage=0.1):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.min_dist = min_dist\n",
        "        self.n_components = n_components\n",
        "        self.n_grids = n_grids\n",
        "        self.sample_percentage = sample_percentage\n",
        "        self.sample_indices = None  # To store sampled indices\n",
        "\n",
        "    def check_categorical(self, data):\n",
        "        return data.select_dtypes(include=['object']).shape[1] > 0\n",
        "\n",
        "    def fit_transform(self, data):\n",
        "        if self.check_categorical(data):\n",
        "            print(\"Categorical variables detected. Using 'dice' metric for UMAP.\")\n",
        "            categorical_data = data.select_dtypes(include=['object'])\n",
        "            encoder = OneHotEncoder(sparse_output=False)\n",
        "            categorical_encoded = encoder.fit_transform(categorical_data)\n",
        "            continuous_data = data.select_dtypes(exclude=['object'])\n",
        "            combined_data = np.hstack([categorical_encoded, continuous_data])\n",
        "            metric = 'dice'\n",
        "        else:\n",
        "            print(\"No categorical variables detected. Using 'euclidean' metric for UMAP.\")\n",
        "            combined_data = data\n",
        "            metric = 'euclidean'\n",
        "\n",
        "        umap_model = umap.UMAP(n_neighbors=self.n_neighbors, min_dist=self.min_dist,\n",
        "                               n_components=self.n_components, metric=metric)\n",
        "        return umap_model.fit_transform(combined_data)\n",
        "\n",
        "    def stratified_sampling(self, embedding):\n",
        "        min_x, min_y = np.min(embedding[:, :2], axis=0)\n",
        "        max_x, max_y = np.max(embedding[:, :2], axis=0)\n",
        "\n",
        "        x_bins = np.linspace(min_x, max_x, self.n_grids)\n",
        "        y_bins = np.linspace(min_y, max_y, self.n_grids)\n",
        "\n",
        "        if self.n_components == 3:\n",
        "            min_z = np.min(embedding[:, 2])\n",
        "            max_z = np.max(embedding[:, 2])\n",
        "            z_bins = np.linspace(min_z, max_z, self.n_grids)\n",
        "            grid_counts = np.zeros((self.n_grids, self.n_grids, self.n_grids))\n",
        "        else:\n",
        "            grid_counts = np.zeros((self.n_grids, self.n_grids))\n",
        "\n",
        "        for point in embedding:\n",
        "            x_idx = np.digitize(point[0], x_bins) - 1\n",
        "            y_idx = np.digitize(point[1], y_bins) - 1\n",
        "\n",
        "            if self.n_components == 3:\n",
        "                z_idx = np.digitize(point[2], z_bins) - 1\n",
        "                grid_counts[x_idx, y_idx, z_idx] += 1\n",
        "            else:\n",
        "                grid_counts[x_idx, y_idx] += 1\n",
        "\n",
        "        grid_probs = grid_counts / np.sum(grid_counts)\n",
        "        grid_probs_flat = grid_probs.flatten()\n",
        "\n",
        "        # Sample indices based on the probability distribution\n",
        "        sampled_indices = np.random.choice(len(embedding), size=int(len(embedding) * self.sample_percentage), replace=False)\n",
        "\n",
        "        # Store the sampled indices for later retrieval\n",
        "        self.sample_indices = sampled_indices\n",
        "\n",
        "        # Extract the corresponding samples from the embedding\n",
        "        sampled_embedding = embedding[sampled_indices]\n",
        "\n",
        "        return sampled_embedding\n",
        "\n",
        "    def get_sample_indices(self):\n",
        "        \"\"\"\n",
        "        Return the indices of the sampled data points in the original dataset.\n",
        "        \"\"\"\n",
        "        if self.sample_indices is None:\n",
        "            raise ValueError(\"No samples have been selected. Please run stratified_sampling first.\")\n",
        "        return self.sample_indices\n",
        "\n",
        "    def plot(self, embedding):\n",
        "        if self.n_components == 2:\n",
        "            plt.scatter(embedding[:, 0], embedding[:, 1], c='blue', marker='o')\n",
        "            plt.title('UMAP Projection (2D)')\n",
        "            plt.xlabel('UMAP 1')\n",
        "            plt.ylabel('UMAP 2')\n",
        "            plt.show()\n",
        "\n",
        "        elif self.n_components == 3:\n",
        "            fig = plt.figure()\n",
        "            ax = fig.add_subplot(111, projection='3d')\n",
        "            ax.scatter(embedding[:, 0], embedding[:, 1], embedding[:, 2], c='blue', marker='o')\n",
        "            ax.set_title('UMAP Projection (3D)')\n",
        "            ax.set_xlabel('UMAP 1')\n",
        "            ax.set_ylabel('UMAP 2')\n",
        "            ax.set_zlabel('UMAP 3')\n",
        "            plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eao-WLtwwS_7"
      },
      "source": [
        "##### HDBSCAN - CLUSTERING BASED SAMPLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "emjvmMeDMmhj"
      },
      "outputs": [],
      "source": [
        "class ClusterSampler:\n",
        "    def __init__(self, data, sampling_percent=10, **hdbscan_params):\n",
        "        \"\"\"\n",
        "        Initialize the ClusterSampler class.\n",
        "\n",
        "        Parameters:\n",
        "        - data: DataFrame containing the dataset\n",
        "        - sampling_percent: Percentage of points to sample from each cluster (0-100)\n",
        "        - **hdbscan_params: Additional parameters to pass to HDBSCAN\n",
        "        \"\"\"\n",
        "        self.data = self._convert_data_types(data)\n",
        "        self.sampling_percent = sampling_percent\n",
        "        self.is_categorical = self._detect_categorical(data)\n",
        "        self.cluster_labels = None\n",
        "        self.hdbscan_params = hdbscan_params\n",
        "\n",
        "    def _convert_data_types(self, data):\n",
        "        \"\"\"Ensure continuous columns are float64 and categorical columns are object.\"\"\"\n",
        "        continuous_cols = data.select_dtypes(include=['float', 'int']).columns\n",
        "        data[continuous_cols] = data[continuous_cols].astype(np.float64)\n",
        "\n",
        "        categorical_cols = data.select_dtypes(include=['object', 'category']).columns\n",
        "        data[categorical_cols] = data[categorical_cols].astype('object')\n",
        "\n",
        "        return data\n",
        "\n",
        "    def _detect_categorical(self, data):\n",
        "        \"\"\"Detect if the dataset contains categorical features.\"\"\"\n",
        "        return data.select_dtypes(include=['object', 'category']).shape[1] > 0\n",
        "\n",
        "    def _compute_distance_matrix(self):\n",
        "        \"\"\"Compute the distance matrix based on the data type.\"\"\"\n",
        "        if self.is_categorical:\n",
        "            gower_matrix = gower.gower_matrix(self.data)\n",
        "            return gower_matrix.astype(np.float64)\n",
        "        else:\n",
        "            return pairwise_distances(self.data, metric='euclidean')\n",
        "\n",
        "    def fit_clusters(self):\n",
        "        \"\"\"Fit HDBSCAN on the dataset with appropriate distance metric.\"\"\"\n",
        "        distance_matrix = self._compute_distance_matrix()\n",
        "        clusterer = hdbscan.HDBSCAN(metric='precomputed' if self.is_categorical else 'euclidean', **self.hdbscan_params)\n",
        "        self.cluster_labels = clusterer.fit_predict(distance_matrix)\n",
        "\n",
        "    def sample_points(self):\n",
        "        \"\"\"Sample a representative subset of points from each cluster, including noise points as a separate cluster.\"\"\"\n",
        "        if self.cluster_labels is None:\n",
        "            raise ValueError(\"Clusters have not been computed. Call fit_clusters() first.\")\n",
        "\n",
        "        data_with_labels = self.data.copy()\n",
        "        data_with_labels['cluster'] = self.cluster_labels\n",
        "\n",
        "        sampled_indices = []\n",
        "        unique_labels = np.unique(self.cluster_labels)\n",
        "\n",
        "        for cluster_label in unique_labels:\n",
        "            cluster_indices = data_with_labels[data_with_labels['cluster'] == cluster_label].index\n",
        "            sample_size = max(1, int(len(cluster_indices) * (self.sampling_percent / 100)))\n",
        "\n",
        "            # Avoid sampling more points than available\n",
        "            if len(cluster_indices) < sample_size:\n",
        "                print(f\"Cluster {cluster_label} has only {len(cluster_indices)} points, sampling {len(cluster_indices)}.\")\n",
        "            sampled_indices.extend(np.random.choice(cluster_indices, min(sample_size, len(cluster_indices)), replace=False))\n",
        "\n",
        "        # Ensure sampled indices are unique and valid\n",
        "        sampled_indices = list(set(sampled_indices))\n",
        "        sampled_indices = [idx for idx in sampled_indices if idx < len(self.data)]\n",
        "\n",
        "        return sampled_indices\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Execute the full sampling pipeline: cluster, then sample.\"\"\"\n",
        "        self.fit_clusters()\n",
        "        return self.sample_points()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGWSKfBWxJNc"
      },
      "source": [
        "##### ISOLATION FOREST AND KS STATISTIC SAMPLER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "t4BSBb9W-9kc"
      },
      "outputs": [],
      "source": [
        "class AnomalySampler:\n",
        "    def __init__(self, X, sample_weight=None):\n",
        "        \"\"\"\n",
        "        Initialize the AnomalySampler class.\n",
        "\n",
        "        Parameters:\n",
        "        - X: DataFrame containing the dataset\n",
        "        - sample_weight: Optional sample weights for the Isolation Forest\n",
        "        \"\"\"\n",
        "        self.original_data = X\n",
        "        self.X = self._one_hot_encode(X)\n",
        "        self.sample_weight = sample_weight\n",
        "        self.preds = self.isolation_forest(self.X, sample_weight)\n",
        "\n",
        "    def _one_hot_encode(self, data):\n",
        "        \"\"\"\n",
        "        Apply one-hot encoding to categorical columns.\n",
        "\n",
        "        Parameters:\n",
        "        - data: DataFrame containing the dataset\n",
        "\n",
        "        Returns:\n",
        "        - One-hot encoded DataFrame\n",
        "        \"\"\"\n",
        "        return pd.get_dummies(data, drop_first=True)  # drop_first to avoid multicollinearity, if relevant\n",
        "\n",
        "    @staticmethod\n",
        "    def isolation_forest(X, sample_weight=None):\n",
        "        \"\"\"\n",
        "        Fits an Isolation Forest to the dataset and assigns an anomaly score to each sample.\n",
        "\n",
        "        Parameters:\n",
        "        - X: DataFrame or array-like containing the dataset\n",
        "        - sample_weight: Optional sample weights for the Isolation Forest\n",
        "\n",
        "        Returns:\n",
        "        - preds: Anomaly scores for each sample\n",
        "        \"\"\"\n",
        "        clf = IsolationForest().fit(X, sample_weight=sample_weight)\n",
        "        preds = clf.score_samples(X)\n",
        "        return preds\n",
        "\n",
        "    @staticmethod\n",
        "    def get_5_percent(num):\n",
        "        \"\"\"Calculate 5% of a given number.\"\"\"\n",
        "        return round(5 / 100 * num)\n",
        "\n",
        "    def get_5_percent_splits(self, length):\n",
        "        \"\"\"Splits a given length into 5% intervals.\"\"\"\n",
        "        five_percent = self.get_5_percent(length)\n",
        "        return np.arange(five_percent, length, five_percent)\n",
        "\n",
        "    def find_sample_indices(self):\n",
        "        \"\"\"\n",
        "        Finds a sample by comparing the distribution of anomaly scores between the sample\n",
        "        and the original distribution using the KS-test. Starts with a 5% sample, increasing\n",
        "        by 5% increments until a significant sample (p-value > 0.95) is found or a limit is reached.\n",
        "\n",
        "        Returns:\n",
        "        - List of indices representing the sample in the original dataset\n",
        "        \"\"\"\n",
        "        size_splits = self.get_5_percent_splits(len(self.X))\n",
        "        element = 1\n",
        "        iteration = 0\n",
        "\n",
        "        while element < len(size_splits):\n",
        "            sample_size = size_splits[element]\n",
        "            sample_indices = np.random.choice(np.arange(self.preds.size), size=sample_size, replace=False)\n",
        "            sample = np.take(self.preds, sample_indices)\n",
        "\n",
        "            # Check if KS test p-value indicates similar distributions\n",
        "            if ks_2samp(self.preds, sample).pvalue > 0.95:\n",
        "                return sample_indices  # Return indices from the original dataset\n",
        "\n",
        "            iteration += 1\n",
        "            if iteration >= 20:\n",
        "                element += 1\n",
        "                iteration = 0\n",
        "\n",
        "        # If no suitable sample is found, return the last attempted sample indices\n",
        "        return sample_indices\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRaDp-XhxRho"
      },
      "source": [
        "#### ENTROPY SAMPLER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3a80f3-ir6gR"
      },
      "outputs": [],
      "source": [
        "class EntropySampler:\n",
        "    def __init__(self, data, sampling_percent=10, bins=10):\n",
        "        \"\"\"\n",
        "        Initialize the EntropySampler class.\n",
        "\n",
        "        Parameters:\n",
        "        - data: DataFrame containing the dataset\n",
        "        - sampling_percent: Percentage of points to sample based on entropy (0-100)\n",
        "        - bins: Number of bins to use for continuous data entropy calculation\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.sampling_percent = sampling_percent\n",
        "        self.bins = bins\n",
        "        self.entropy_scores = None\n",
        "\n",
        "    def _calculate_entropy(self):\n",
        "        \"\"\"\n",
        "        Calculate the entropy for each feature and aggregate entropy per data point.\n",
        "\n",
        "        Returns:\n",
        "        - entropy_scores: Array of entropy scores for each data point\n",
        "        \"\"\"\n",
        "        entropy_scores = np.zeros(len(self.data))\n",
        "\n",
        "        for col in self.data.columns:\n",
        "            if pd.api.types.is_numeric_dtype(self.data[col]):\n",
        "                # For continuous data, bin it and calculate entropy over the bins\n",
        "                counts, _ = np.histogram(self.data[col], bins=self.bins)\n",
        "                feature_entropy = entropy(counts + 1e-10)  # Add small value to avoid log(0)\n",
        "                feature_contributions = np.digitize(self.data[col], bins=np.histogram_bin_edges(self.data[col], bins=self.bins))\n",
        "            else:\n",
        "                # For categorical data, calculate entropy over unique values\n",
        "                counts = self.data[col].value_counts().values\n",
        "                feature_entropy = entropy(counts + 1e-10)\n",
        "                feature_contributions = self.data[col].map(self.data[col].value_counts(normalize=True)).values\n",
        "\n",
        "            # Accumulate entropy scores based on the feature contributions for each data point\n",
        "            entropy_scores += feature_contributions * feature_entropy\n",
        "\n",
        "        self.entropy_scores = entropy_scores\n",
        "\n",
        "    def sample_indices(self):\n",
        "        \"\"\"\n",
        "        Get indices of points with the highest entropy scores.\n",
        "\n",
        "        Returns:\n",
        "        - List of indices for sampled points based on entropy scores\n",
        "        \"\"\"\n",
        "        if self.entropy_scores is None:\n",
        "            self._calculate_entropy()\n",
        "\n",
        "        # Determine the number of points to sample\n",
        "        num_samples = max(1, int(len(self.data) * (self.sampling_percent / 100)))\n",
        "\n",
        "        # Get the indices of the top entropy scores\n",
        "        top_indices = np.argsort(self.entropy_scores)[-num_samples:]\n",
        "\n",
        "        return top_indices\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Execute the full entropy-based sampling process.\n",
        "\n",
        "        Returns:\n",
        "        - List of indices for sampled points based on entropy scores\n",
        "        \"\"\"\n",
        "        self._calculate_entropy()\n",
        "        return self.sample_indices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8XnIZ2O33UO"
      },
      "source": [
        "### DISTANCE BASED SAMPLER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1qgqGwih34r9"
      },
      "outputs": [],
      "source": [
        "class DistanceBasedSampler:\n",
        "    def __init__(self, data, k=5, sampling_percent=10):\n",
        "        self.data = data.reset_index(drop=True)  # Reset index for consistent access\n",
        "        self.k = k\n",
        "        self.sampling_percent = sampling_percent\n",
        "        self.cluster_labels = None\n",
        "        self.cluster_centroids = None\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        \"\"\"Ensure all categorical columns are treated as strings.\"\"\"\n",
        "        self.data = self.data.apply(lambda col: col.astype(str) if col.dtype == 'object' else col)\n",
        "        return self.data\n",
        "\n",
        "    def fit_clusters(self):\n",
        "        \"\"\"\n",
        "        Fit clusters using K-Prototypes if categorical data is present,\n",
        "        otherwise fallback to K-Means for purely numerical data.\n",
        "        \"\"\"\n",
        "        self._prepare_data()\n",
        "        data_array = self.data.values\n",
        "\n",
        "        # Identify categorical column indices\n",
        "        categorical_indices = [\n",
        "            i for i, col in enumerate(self.data.columns) if self.data.dtypes[col] == 'object'\n",
        "        ]\n",
        "\n",
        "        if len(categorical_indices) == 0:\n",
        "            # No categorical data, use K-Means\n",
        "            print(\"No categorical data detected. Using K-Means instead.\")\n",
        "            kmeans = KMeans(n_clusters=self.k, random_state=42)\n",
        "            clusters = kmeans.fit_predict(data_array)\n",
        "            self.cluster_labels = clusters\n",
        "            self.cluster_centroids = kmeans.cluster_centers_\n",
        "        else:\n",
        "            # Use K-Prototypes for mixed data\n",
        "            kproto = KPrototypes(n_clusters=self.k, init='Cao', random_state=42)\n",
        "            clusters = kproto.fit_predict(data_array, categorical=categorical_indices)\n",
        "            self.cluster_labels = clusters\n",
        "            self.cluster_centroids = kproto.cluster_centroids_\n",
        "\n",
        "        return clusters\n",
        "\n",
        "    def _calculate_mixed_distance(self, point, centroids):\n",
        "        \"\"\"\n",
        "        Calculate the mixed distance (numerical + categorical) from a point to centroids.\n",
        "        \"\"\"\n",
        "        numeric_cols = self.data.select_dtypes(include=[np.number]).columns\n",
        "        categorical_cols = self.data.select_dtypes(include=['object']).columns\n",
        "\n",
        "        point_numeric = point[numeric_cols].values.astype(float)\n",
        "        point_categorical = point[categorical_cols].values\n",
        "\n",
        "        distances = []\n",
        "        for centroid in centroids:\n",
        "            # Split centroids into numeric and categorical parts\n",
        "            centroid_numeric = np.array(centroid[:len(numeric_cols)], dtype=float)\n",
        "            centroid_categorical = np.array(centroid[len(numeric_cols):])\n",
        "\n",
        "            # Calculate numerical distance (Euclidean)\n",
        "            numeric_distance = np.linalg.norm(point_numeric - centroid_numeric)\n",
        "            # Calculate categorical distance (Hamming)\n",
        "            categorical_distance = np.sum(point_categorical != centroid_categorical)\n",
        "\n",
        "            distances.append(numeric_distance + categorical_distance)\n",
        "\n",
        "        return np.array(distances)\n",
        "\n",
        "    def sample_indices(self):\n",
        "        \"\"\"\n",
        "        Identify sample indices based on distance to centroids.\n",
        "        \"\"\"\n",
        "        if self.cluster_labels is None:\n",
        "            self.fit_clusters()\n",
        "\n",
        "        centroids = self.cluster_centroids\n",
        "        distances = np.zeros(len(self.data))\n",
        "\n",
        "        for i, (_, point) in enumerate(self.data.iterrows()):\n",
        "            distances[i] = np.min(self._calculate_mixed_distance(point, centroids))\n",
        "\n",
        "        num_samples = max(1, int(len(self.data) * (self.sampling_percent / 100)))\n",
        "        sampled_indices = np.argsort(distances)[-num_samples:]\n",
        "\n",
        "        return sampled_indices\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Run the sampling process.\n",
        "        \"\"\"\n",
        "        self.fit_clusters()\n",
        "        return self.sample_indices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5w6E43q4zYH"
      },
      "source": [
        "#### RANDOM SAMPLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lZclmsfb4vgc"
      },
      "outputs": [],
      "source": [
        "class RandomSampler:\n",
        "    def __init__(self, data, sampling_percent=10):\n",
        "        \"\"\"\n",
        "        Initialize the RandomSampler class.\n",
        "\n",
        "        Parameters:\n",
        "        - data: DataFrame containing the dataset\n",
        "        - sampling_percent: Percentage of rows to sample (0-100)\n",
        "        \"\"\"\n",
        "        self.data = data.reset_index(drop=True)  # Reset index to ensure consistency\n",
        "        self.sampling_percent = sampling_percent\n",
        "\n",
        "    def sample_indices(self):\n",
        "        \"\"\"\n",
        "        Randomly sample indices of rows from the dataset.\n",
        "\n",
        "        Returns:\n",
        "        - A list of sampled indices.\n",
        "        \"\"\"\n",
        "        if not (0 <= self.sampling_percent <= 100):\n",
        "            raise ValueError(\"sampling_percent must be between 0 and 100.\")\n",
        "\n",
        "        # Calculate the number of samples to draw\n",
        "        sample_size = int(len(self.data) * (self.sampling_percent / 100))\n",
        "        sample_size = max(1, sample_size)  # Ensure at least one index is selected\n",
        "\n",
        "        # Perform random sampling without replacement and return indices\n",
        "        sampled_indices = self.data.sample(n=sample_size, random_state=42).index.tolist()\n",
        "        return sampled_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C74qxcZZ42Rz"
      },
      "source": [
        "### MACHINE LEARNING MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9M0kBWKKsi6N"
      },
      "outputs": [],
      "source": [
        "# !mkdir Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuZdfS3kN2OZ"
      },
      "source": [
        "#### READING DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VNwWlcPn-eqD"
      },
      "outputs": [],
      "source": [
        "# Custom transformer to drop highly correlated features\n",
        "class DropHighCorrelation(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, threshold=0.6):\n",
        "        \"\"\"\n",
        "        Custom transformer to drop highly correlated features.\n",
        "\n",
        "        Parameters:\n",
        "        - threshold: Correlation threshold above which features will be dropped.\n",
        "        \"\"\"\n",
        "        self.threshold = threshold\n",
        "        self.to_drop = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Identify features to drop based on the correlation threshold.\n",
        "        \"\"\"\n",
        "        corr_matrix = pd.DataFrame(X).corr().abs()  # Compute the absolute correlation matrix\n",
        "        upper_tri = corr_matrix.where(\n",
        "            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
        "        )  # Extract upper triangle\n",
        "        self.to_drop = [\n",
        "            column for column in upper_tri.columns if any(upper_tri[column] > self.threshold)\n",
        "        ]\n",
        "\n",
        "        print(f\"DropHighCorrelation: {len(self.to_drop)} columns will be dropped due to correlation (threshold={self.threshold}).\")\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Drop the identified features from the dataset.\n",
        "        \"\"\"\n",
        "        return pd.DataFrame(X).drop(columns=self.to_drop, errors='ignore')\n",
        "\n",
        "# Main ModelTrainer class\n",
        "class ModelTrainer:\n",
        "    def __init__(self, datasets_X, datasets_y, task_type, dataset_names=None):\n",
        "        \"\"\"\n",
        "        Initializes the ModelTrainer with datasets, explicitly provided task type, and dataset names.\n",
        "\n",
        "        Parameters:\n",
        "        - datasets_X: List of [X_train, X_test] for different datasets\n",
        "        - datasets_y: List of [y_train, y_test] for different datasets\n",
        "        - task_type: A string explicitly specifying the task type, either 'regression' or 'classification'.\n",
        "        - dataset_names: List of names for the datasets to be used as index in the results DataFrame\n",
        "        \"\"\"\n",
        "        if task_type not in ['classification', 'regression']:\n",
        "            raise ValueError(\"task_type must be either 'classification' or 'regression'\")\n",
        "\n",
        "        self.datasets_X = datasets_X\n",
        "        self.datasets_y = datasets_y\n",
        "        self.task_type = task_type\n",
        "        self.dataset_names = dataset_names\n",
        "\n",
        "    def _select_model(self):\n",
        "        \"\"\"Select model based on task type.\"\"\"\n",
        "        if self.task_type == 'classification':\n",
        "            return {\n",
        "                'RandomForest': RandomForestClassifier(random_state=42),\n",
        "                'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
        "                'LogisticRegression': LogisticRegression(),\n",
        "                'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "            }\n",
        "        elif self.task_type == 'regression':\n",
        "            return {\n",
        "                'RandomForest': RandomForestRegressor(random_state=42),\n",
        "                'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
        "                'LinearRegression': LinearRegression(),\n",
        "                'XGBoost': xgb.XGBRegressor(random_state=42)\n",
        "            }\n",
        "\n",
        "    def _create_pipeline(self, model, X):\n",
        "        \"\"\"Create a preprocessing and modeling pipeline.\"\"\"\n",
        "        # Identify categorical and numerical features\n",
        "        categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "        numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "        # Preprocessing for numerical features: Standard scaling\n",
        "        numerical_transformer = StandardScaler()\n",
        "\n",
        "        # Preprocessing for categorical features: One-hot encoding\n",
        "        categorical_transformer = OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False)\n",
        "\n",
        "        # Combine preprocessors in a column transformer\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numerical_transformer, numerical_features),\n",
        "                ('cat', categorical_transformer, categorical_features)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Define a pipeline with preprocessing, correlation dropping, and the specified model\n",
        "        pipeline = Pipeline(steps=[\n",
        "            ('preprocessor', preprocessor),  # First preprocess\n",
        "            ('drop_high_corr', DropHighCorrelation(threshold=0.6)),  # Then drop highly correlated features\n",
        "            ('model', model)  # Finally, apply the model\n",
        "        ])\n",
        "        return pipeline\n",
        "\n",
        "    def _get_best_cutoff(self, y_true, y_pred_proba):\n",
        "        \"\"\"Use Youden's J statistic to determine the best cutoff point for classification.\"\"\"\n",
        "        fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
        "        youden_index = tpr - fpr\n",
        "        best_cutoff = thresholds[np.argmax(youden_index)]\n",
        "        return best_cutoff\n",
        "\n",
        "    def _train_and_evaluate(self, model, X_train, X_test, y_train, y_test):\n",
        "        \"\"\"Train the model and evaluate it on both the training and test datasets.\"\"\"\n",
        "        start_time = time.time()\n",
        "        model.fit(X_train, y_train)\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        y_pred_train = model.predict(X_train)\n",
        "        y_pred_test = model.predict(X_test)\n",
        "\n",
        "        if self.task_type == 'classification':\n",
        "            y_pred_proba_test = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else y_pred_test\n",
        "            y_pred_proba_train = model.predict_proba(X_train)[:, 1] if hasattr(model, 'predict_proba') else y_pred_train\n",
        "\n",
        "            best_cutoff = self._get_best_cutoff(y_test, y_pred_proba_test)\n",
        "            y_pred_class_test = (y_pred_proba_test >= best_cutoff).astype(int)\n",
        "            y_pred_class_train = (y_pred_proba_train >= best_cutoff).astype(int)\n",
        "\n",
        "            metrics = {\n",
        "                'Train F1': f1_score(y_train, y_pred_class_train),\n",
        "                'Test F1': f1_score(y_test, y_pred_class_test),\n",
        "                'Train AUC': roc_auc_score(y_train, y_pred_proba_train),\n",
        "                'Test AUC': roc_auc_score(y_test, y_pred_proba_test),\n",
        "                'Train Accuracy': accuracy_score(y_train, y_pred_class_train),\n",
        "                'Test Accuracy': accuracy_score(y_test, y_pred_class_test),\n",
        "                'Train Recall': recall_score(y_train, y_pred_class_train),\n",
        "                'Test Recall': recall_score(y_test, y_pred_class_test),\n",
        "                'Train Precision': precision_score(y_train, y_pred_class_train),\n",
        "                'Test Precision': precision_score(y_test, y_pred_class_test),\n",
        "                'Best Cutoff': best_cutoff,\n",
        "                'Training Time (seconds)': training_time\n",
        "            }\n",
        "        else:\n",
        "            metrics = {\n",
        "                'Train MSE': mean_squared_error(y_train, y_pred_train),\n",
        "                'Test MSE': mean_squared_error(y_test, y_pred_test),\n",
        "                'Train MAPE': mean_absolute_percentage_error(y_train, y_pred_train),\n",
        "                'Test MAPE': mean_absolute_percentage_error(y_test, y_pred_test),\n",
        "                'Train R2': r2_score(y_train, y_pred_train),\n",
        "                'Test R2': r2_score(y_test, y_pred_test),\n",
        "                'Training Time (seconds)': training_time\n",
        "            }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def train_models(self):\n",
        "        \"\"\"Train and evaluate models on multiple datasets and return a DataFrame of results.\"\"\"\n",
        "        models = self._select_model()\n",
        "        results = []\n",
        "\n",
        "        for idx, (X_data, y_data) in enumerate(zip(self.datasets_X, self.datasets_y)):\n",
        "            X_train, X_test = X_data\n",
        "            y_train, y_test = y_data\n",
        "\n",
        "            for model_name, model in models.items():\n",
        "                pipeline = self._create_pipeline(model, X_train)\n",
        "                metrics = self._train_and_evaluate(pipeline, X_train, X_test, y_train, y_test)\n",
        "                metrics['Dataset'] = self.dataset_names[idx] if self.dataset_names else f'Dataset {idx+1}'\n",
        "                metrics['Model'] = model_name\n",
        "                results.append(metrics)\n",
        "\n",
        "        return pd.DataFrame(results).set_index('Dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "5m4zGB5rsjZU",
        "outputId": "d54b15ff-2f5c-4711-ce52-e1e3efa647a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   ACCESS_ALL_DOWNLOADS  ACCESS_CACHE_FILESYSTEM  ACCESS_CHECKIN_PROPERTIES  \\\n",
              "0                   0.0                      0.0                        0.0   \n",
              "1                   0.0                      0.0                        0.0   \n",
              "2                   0.0                      0.0                        0.0   \n",
              "3                   0.0                      0.0                        0.0   \n",
              "4                   0.0                      0.0                        0.0   \n",
              "\n",
              "   ACCESS_COARSE_LOCATION  ACCESS_COARSE_UPDATES  ACCESS_FINE_LOCATION  \\\n",
              "0                     0.0                    0.0                   0.0   \n",
              "1                     0.0                    0.0                   0.0   \n",
              "2                     0.0                    0.0                   0.0   \n",
              "3                     0.0                    0.0                   0.0   \n",
              "4                     0.0                    0.0                   0.0   \n",
              "\n",
              "   ACCESS_LOCATION_EXTRA_COMMANDS  ACCESS_MOCK_LOCATION  ACCESS_MTK_MMHW  \\\n",
              "0                             0.0                   0.0              0.0   \n",
              "1                             0.0                   0.0              0.0   \n",
              "2                             0.0                   0.0              0.0   \n",
              "3                             0.0                   0.0              0.0   \n",
              "4                             0.0                   0.0              0.0   \n",
              "\n",
              "   ACCESS_NETWORK_STATE  ...  \\\n",
              "0                   1.0  ...   \n",
              "1                   1.0  ...   \n",
              "2                   1.0  ...   \n",
              "3                   0.0  ...   \n",
              "4                   0.0  ...   \n",
              "\n",
              "   Landroid_content_pm_PackageManager___getInstalledPackages  \\\n",
              "0                                                0.0           \n",
              "1                                                0.0           \n",
              "2                                                0.0           \n",
              "3                                                0.0           \n",
              "4                                                0.0           \n",
              "\n",
              "   Landroid_telephony_TelephonyManager___getLine1Number  \\\n",
              "0                                                1.0      \n",
              "1                                                0.0      \n",
              "2                                                0.0      \n",
              "3                                                0.0      \n",
              "4                                                0.0      \n",
              "\n",
              "   Landroid_telephony_TelephonyManager___getNetworkOperator  \\\n",
              "0                                                1.0          \n",
              "1                                                0.0          \n",
              "2                                                0.0          \n",
              "3                                                1.0          \n",
              "4                                                0.0          \n",
              "\n",
              "   Landroid_telephony_TelephonyManager___getNetworkOperatorName  \\\n",
              "0                                                1.0              \n",
              "1                                                0.0              \n",
              "2                                                0.0              \n",
              "3                                                1.0              \n",
              "4                                                0.0              \n",
              "\n",
              "   Landroid_telephony_TelephonyManager___getNetworkCountryIso  \\\n",
              "0                                                0.0            \n",
              "1                                                1.0            \n",
              "2                                                0.0            \n",
              "3                                                1.0            \n",
              "4                                                0.0            \n",
              "\n",
              "   Landroid_telephony_TelephonyManager___getSimOperator  \\\n",
              "0                                                0.0      \n",
              "1                                                0.0      \n",
              "2                                                0.0      \n",
              "3                                                1.0      \n",
              "4                                                0.0      \n",
              "\n",
              "   Landroid_telephony_TelephonyManager___getSimOperatorName  \\\n",
              "0                                                0.0          \n",
              "1                                                0.0          \n",
              "2                                                0.0          \n",
              "3                                                0.0          \n",
              "4                                                0.0          \n",
              "\n",
              "   Landroid_telephony_TelephonyManager___getSimCountryIso  \\\n",
              "0                                                0.0        \n",
              "1                                                1.0        \n",
              "2                                                0.0        \n",
              "3                                                1.0        \n",
              "4                                                0.0        \n",
              "\n",
              "   Landroid_telephony_TelephonyManager___getSimSerialNumber  \\\n",
              "0                                                0.0          \n",
              "1                                                0.0          \n",
              "2                                                0.0          \n",
              "3                                                0.0          \n",
              "4                                                0.0          \n",
              "\n",
              "   Lorg_apache_http_impl_client_DefaultHttpClient___execute  \n",
              "0                                                1.0         \n",
              "1                                                0.0         \n",
              "2                                                0.0         \n",
              "3                                                0.0         \n",
              "4                                                0.0         \n",
              "\n",
              "[5 rows x 241 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8a9af07c-f64d-4ec3-afef-58b79a702648\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ACCESS_ALL_DOWNLOADS</th>\n",
              "      <th>ACCESS_CACHE_FILESYSTEM</th>\n",
              "      <th>ACCESS_CHECKIN_PROPERTIES</th>\n",
              "      <th>ACCESS_COARSE_LOCATION</th>\n",
              "      <th>ACCESS_COARSE_UPDATES</th>\n",
              "      <th>ACCESS_FINE_LOCATION</th>\n",
              "      <th>ACCESS_LOCATION_EXTRA_COMMANDS</th>\n",
              "      <th>ACCESS_MOCK_LOCATION</th>\n",
              "      <th>ACCESS_MTK_MMHW</th>\n",
              "      <th>ACCESS_NETWORK_STATE</th>\n",
              "      <th>...</th>\n",
              "      <th>Landroid_content_pm_PackageManager___getInstalledPackages</th>\n",
              "      <th>Landroid_telephony_TelephonyManager___getLine1Number</th>\n",
              "      <th>Landroid_telephony_TelephonyManager___getNetworkOperator</th>\n",
              "      <th>Landroid_telephony_TelephonyManager___getNetworkOperatorName</th>\n",
              "      <th>Landroid_telephony_TelephonyManager___getNetworkCountryIso</th>\n",
              "      <th>Landroid_telephony_TelephonyManager___getSimOperator</th>\n",
              "      <th>Landroid_telephony_TelephonyManager___getSimOperatorName</th>\n",
              "      <th>Landroid_telephony_TelephonyManager___getSimCountryIso</th>\n",
              "      <th>Landroid_telephony_TelephonyManager___getSimSerialNumber</th>\n",
              "      <th>Lorg_apache_http_impl_client_DefaultHttpClient___execute</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 241 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a9af07c-f64d-4ec3-afef-58b79a702648')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8a9af07c-f64d-4ec3-afef-58b79a702648 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8a9af07c-f64d-4ec3-afef-58b79a702648');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "train = pd.read_csv('/content/Data/TUANDROMD.csv')\n",
        "\n",
        "# Display the first few rows of the training data\n",
        "print(\"Training Data:\")\n",
        "\n",
        "train = train[~(train['Label'].isnull())]\n",
        "train.columns = train.columns.str.replace(r'[^a-zA-Z0-9_]', '_', regex=True)\n",
        "\n",
        "X = train.drop(columns=['Label'])\n",
        "y = train['Label']\n",
        "\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "# Convert categorical features to category dtype\n",
        "for feature in categorical_features:\n",
        "    X[feature] = X[feature].astype('category')\n",
        "\n",
        "X.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "r-r4zOJIwkmp"
      },
      "outputs": [],
      "source": [
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTMpo3HS4Drb",
        "outputId": "3f50db96-3516-4967-9330-43601d318feb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No categorical variables detected. Using 'euclidean' metric for UMAP.\n",
            "Sampled Data: 1071 Data: 3571\n",
            "Time taken for Dim Red and Grid Search Sampling: 17.04332399368286 seconds\n",
            "Sampled Data: 358 Data: 3571\n",
            "Time taken for Isolation Forest and KS Sampling: 0.14914226531982422 seconds\n",
            "Sampled Data: 1071 Data: 3571\n",
            "Time taken for Entropy Sampling: 0.15228939056396484 seconds\n",
            "No categorical data detected. Using K-Means instead.\n",
            "Sampled Data: 1071 Data: 3571\n",
            "Time taken for Distance Based Sampling: 3.088894844055176 seconds\n",
            "Sampled Data: 1071 Data: 3571\n",
            "Time taken for Random Sampling: 0.005179643630981445 seconds\n"
          ]
        }
      ],
      "source": [
        "#### SAMPLING METHODS ####\n",
        "data = X_train1\n",
        "# ================================ #\n",
        "##### DIM REDUCTION AND GRID SEARCH\n",
        "# ================================ #\n",
        "start_time = time.time()\n",
        "stratified_sampler = StratifiedSamplingUMAP(n_neighbors=15, min_dist=0.1, n_components=3, n_grids=10, sample_percentage=0.3)\n",
        "embedding = stratified_sampler.fit_transform(data)\n",
        "sampled_embedding = stratified_sampler.stratified_sampling(embedding)\n",
        "sample_indices = stratified_sampler.get_sample_indices()\n",
        "print(\"Sampled Data:\", len(sample_indices),\"Data:\",len(data))\n",
        "X_train1_1 = data.iloc[sample_indices]\n",
        "y_train1_1 = y_train1.iloc[sample_indices]\n",
        "print(\"Time taken for Dim Red and Grid Search Sampling:\", time.time() - start_time, \"seconds\")\n",
        "\n",
        "# # ================================ #\n",
        "# ##### HDBSCAN - CLUSTERING BASED SAMPLING\n",
        "# # ================================ #\n",
        "\n",
        "# start_time = time.time()\n",
        "# sampler = ClusterSampler(data, sampling_percent=30, min_cluster_size=15, min_samples=3)\n",
        "# sampled_indices = sampler.run()\n",
        "# X_train1_2 = data.loc[sampled_indices]\n",
        "# y_train1_2 = y_train1.iloc[sampled_indices]\n",
        "# print(\"Sampled Data:\", len(sampled_indices),\"Data:\",len(data))\n",
        "# print(\"Time taken for Clustering Based Sampling:\", time.time() - start_time, \"seconds\")\n",
        "\n",
        "# ================================ #\n",
        "##### ISOLATION FOREST AND KS STATISTIC SAMPLER #####\n",
        "# ================================ #\n",
        "\n",
        "start_time = time.time()\n",
        "sampler = AnomalySampler(data)\n",
        "sample_indices = sampler.find_sample_indices()\n",
        "X_train1_3 = data.iloc[sample_indices]\n",
        "y_train1_3 = y_train1.iloc[sample_indices]\n",
        "print(\"Sampled Data:\", len(sample_indices),\"Data:\",len(data))\n",
        "print(\"Time taken for Isolation Forest and KS Sampling:\", time.time() - start_time, \"seconds\")\n",
        "\n",
        "# ================================ #\n",
        "##### ENTROPY SAMPLER #####\n",
        "# ================================ #\n",
        "\n",
        "start_time = time.time()\n",
        "sampler = EntropySampler(data,sampling_percent=30)\n",
        "sample_indices = sampler.run()\n",
        "X_train1_4 = data.iloc[sample_indices]\n",
        "y_train1_4 = y_train1.iloc[sample_indices]\n",
        "print(\"Sampled Data:\", len(sample_indices),\"Data:\",len(data))\n",
        "print(\"Time taken for Entropy Sampling:\", time.time() - start_time, \"seconds\")\n",
        "\n",
        "# ================================ #\n",
        "##### DISTANCE BASED SAMPLER #####\n",
        "# ================================ #\n",
        "\n",
        "start_time = time.time()\n",
        "sampler = DistanceBasedSampler(data, k=5, sampling_percent=30)\n",
        "sampled_indices = sampler.run()\n",
        "X_train1_5 = data.iloc[sampled_indices]\n",
        "y_train1_5 = y_train1.iloc[sampled_indices]\n",
        "print(\"Sampled Data:\", len(sampled_indices),\"Data:\",len(data))\n",
        "print(\"Time taken for Distance Based Sampling:\", time.time() - start_time, \"seconds\")\n",
        "\n",
        "# ================================ #\n",
        "##### RANDOM SAMPLER #####\n",
        "# ================================ #\n",
        "\n",
        "start_time = time.time()\n",
        "sampler = RandomSampler(data=data, sampling_percent=30)\n",
        "sampled_indices = sampler.sample_indices()\n",
        "X_train1_6 = X_train1.iloc[sampled_indices]\n",
        "y_train1_6 = y_train1.iloc[sampled_indices]\n",
        "print(\"Sampled Data:\", len(sampled_indices),\"Data:\",len(data))\n",
        "print(\"Time taken for Random Sampling:\", time.time() - start_time, \"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrPoyf0h4Hrv",
        "outputId": "2ce91258-d325-4dd5-b780-6abb317512ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DropHighCorrelation: 158 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 158 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 158 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 158 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 153 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 153 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 153 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 153 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 151 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 151 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 151 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 151 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 152 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 152 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 152 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 152 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 156 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 156 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 156 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 156 columns will be dropped due to correlation (threshold=0.6).\n"
          ]
        }
      ],
      "source": [
        "# ================================ #\n",
        "##### Trainer for All State Sampling Methods\n",
        "# ================================ #\n",
        "trainer = ModelTrainer(\n",
        "    datasets_X=[\n",
        "        [X_train1_1, X_test1],\n",
        "        # [X_train1_2, X_test1],\n",
        "        [X_train1_3, X_test1],\n",
        "        [X_train1_4, X_test1],\n",
        "        [X_train1_5, X_test1],\n",
        "        [X_train1_6, X_test1]\n",
        "    ],\n",
        "    datasets_y=[\n",
        "        [y_train1_1, y_test1],\n",
        "        # [y_train1_2, y_test1],\n",
        "        [y_train1_3, y_test1],\n",
        "        [y_train1_4, y_test1],\n",
        "        [y_train1_5, y_test1],\n",
        "        [y_train1_6, y_test1]\n",
        "    ],\n",
        "    task_type='classification',\n",
        "    dataset_names=[\n",
        "        'Dim Reduction',\n",
        "        'Clustering',\n",
        "        'Anomaly Detection',\n",
        "        'Entropy',\n",
        "        'Distance Based',\n",
        "        'Random'\n",
        "    ]\n",
        ")\n",
        "\n",
        "results = trainer.train_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hJNQQkUc60mx"
      },
      "outputs": [],
      "source": [
        "results.to_csv(\"TUANDROMD.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "id": "QtjGJOybmDxi",
        "outputId": "66bc35c6-4495-4144-ea0d-f00115017529"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Train F1   Test F1  Train AUC  Test AUC  Train Accuracy  \\\n",
              "Dataset                                                                      \n",
              "Dim Reduction      0.992883  0.985856   0.998255  0.996369        0.988796   \n",
              "Dim Reduction      0.988081  0.983700   0.997821  0.990330        0.981326   \n",
              "Dim Reduction      0.985731  0.979505   0.995046  0.979640        0.977591   \n",
              "Dim Reduction      0.989286  0.983723   0.997847  0.991565        0.983193   \n",
              "Clustering         0.996466  0.983769   0.998763  0.990338        0.994413   \n",
              "Clustering         0.996466  0.980892   0.999334  0.984716        0.994413   \n",
              "Clustering         0.989362  0.976711   0.997859  0.983797        0.983240   \n",
              "Clustering         0.980322  0.975852   0.998239  0.974883        0.969274   \n",
              "Anomaly Detection  1.000000  0.962319   1.000000  0.984362        1.000000   \n",
              "Anomaly Detection  0.996673  0.957091   0.999834  0.958326        0.995331   \n",
              "Anomaly Detection  0.986649  0.953890   0.998700  0.946922        0.981326   \n",
              "Anomaly Detection  0.997996  0.957787   1.000000  0.962416        0.997199   \n",
              "Entropy            0.994633  0.983927   0.999708  0.978370        0.994398   \n",
              "Entropy            0.941476  0.972822   0.996782  0.967734        0.935574   \n",
              "Entropy            0.922034  0.955056   0.987084  0.937588        0.914099   \n",
              "Entropy            0.934454  0.972145   0.999778  0.965498        0.927171   \n",
              "Distance Based     0.991140  0.980756   0.999531  0.996295        0.985994   \n",
              "Distance Based     0.990544  0.982232   0.999277  0.990393        0.985061   \n",
              "Distance Based     0.989399  0.980254   0.997372  0.980665        0.983193   \n",
              "Distance Based     0.989362  0.981534   0.999325  0.992485        0.983193   \n",
              "\n",
              "                   Test Accuracy  Train Recall  Test Recall  Train Precision  \\\n",
              "Dataset                                                                        \n",
              "Dim Reduction           0.977604      0.988194     0.977560         0.997616   \n",
              "Dim Reduction           0.974244      0.978749     0.973352         0.997593   \n",
              "Dim Reduction           0.967525      0.978749     0.971950         0.992814   \n",
              "Dim Reduction           0.974244      0.981110     0.974755         0.997599   \n",
              "Clustering              0.974244      0.992958     0.977560         1.000000   \n",
              "Clustering              0.969765      0.992958     0.971950         1.000000   \n",
              "Clustering              0.963046      0.982394     0.970547         0.996429   \n",
              "Clustering              0.961926      0.964789     0.963534         0.996364   \n",
              "Anomaly Detection       0.941769      1.000000     0.931276         1.000000   \n",
              "Anomaly Detection       0.933931      0.998667     0.922861         0.994688   \n",
              "Anomaly Detection       0.928331      0.985333     0.928471         0.987968   \n",
              "Anomaly Detection       0.935050      0.996000     0.922861         1.000000   \n",
              "Entropy                 0.974244      1.000000     0.987377         0.989324   \n",
              "Entropy                 0.956327      0.998201     0.978962         0.890851   \n",
              "Entropy                 0.928331      0.978417     0.953717         0.871795   \n",
              "Entropy                 0.955207      1.000000     0.978962         0.876972   \n",
              "Distance Based          0.969765      0.982436     0.964937         1.000000   \n",
              "Distance Based          0.972004      0.981265     0.969144         1.000000   \n",
              "Distance Based          0.968645      0.983607     0.974755         0.995261   \n",
              "Distance Based          0.970885      0.980094     0.969144         0.998807   \n",
              "\n",
              "                   Test Precision  Best Cutoff  Training Time (seconds)  \\\n",
              "Dataset                                                                   \n",
              "Dim Reduction            0.994294     0.650000                 0.261065   \n",
              "Dim Reduction            0.994269     0.877098                 0.265567   \n",
              "Dim Reduction            0.987179     0.807729                 0.143178   \n",
              "Dim Reduction            0.992857     0.809253                 0.297886   \n",
              "Clustering               0.990057     0.600000                 0.172153   \n",
              "Clustering               0.990000     0.935731                 0.155137   \n",
              "Clustering               0.982955     0.814117                 0.069192   \n",
              "Clustering               0.988489     0.938267                 0.186601   \n",
              "Anomaly Detection        0.995502     0.430000                 0.256536   \n",
              "Anomaly Detection        0.993958     0.485456                 0.271453   \n",
              "Anomaly Detection        0.980741     0.408349                 0.142090   \n",
              "Anomaly Detection        0.995461     0.818739                 0.231161   \n",
              "Entropy                  0.980501     0.250000                 0.250904   \n",
              "Entropy                  0.966759     0.116102                 0.276588   \n",
              "Entropy                  0.956399     0.199447                 0.142381   \n",
              "Entropy                  0.965422     0.019008                 0.243313   \n",
              "Distance Based           0.997101     0.730000                 0.247783   \n",
              "Distance Based           0.995677     0.887572                 0.265048   \n",
              "Distance Based           0.985816     0.645286                 0.141880   \n",
              "Distance Based           0.994245     0.796098                 0.229198   \n",
              "\n",
              "                                Model  \n",
              "Dataset                                \n",
              "Dim Reduction            RandomForest  \n",
              "Dim Reduction        GradientBoosting  \n",
              "Dim Reduction      LogisticRegression  \n",
              "Dim Reduction                 XGBoost  \n",
              "Clustering               RandomForest  \n",
              "Clustering           GradientBoosting  \n",
              "Clustering         LogisticRegression  \n",
              "Clustering                    XGBoost  \n",
              "Anomaly Detection        RandomForest  \n",
              "Anomaly Detection    GradientBoosting  \n",
              "Anomaly Detection  LogisticRegression  \n",
              "Anomaly Detection             XGBoost  \n",
              "Entropy                  RandomForest  \n",
              "Entropy              GradientBoosting  \n",
              "Entropy            LogisticRegression  \n",
              "Entropy                       XGBoost  \n",
              "Distance Based           RandomForest  \n",
              "Distance Based       GradientBoosting  \n",
              "Distance Based     LogisticRegression  \n",
              "Distance Based                XGBoost  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-73784d8f-7e93-47f1-86ea-cf5a0aabc5ce\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train F1</th>\n",
              "      <th>Test F1</th>\n",
              "      <th>Train AUC</th>\n",
              "      <th>Test AUC</th>\n",
              "      <th>Train Accuracy</th>\n",
              "      <th>Test Accuracy</th>\n",
              "      <th>Train Recall</th>\n",
              "      <th>Test Recall</th>\n",
              "      <th>Train Precision</th>\n",
              "      <th>Test Precision</th>\n",
              "      <th>Best Cutoff</th>\n",
              "      <th>Training Time (seconds)</th>\n",
              "      <th>Model</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dataset</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Dim Reduction</th>\n",
              "      <td>0.992883</td>\n",
              "      <td>0.985856</td>\n",
              "      <td>0.998255</td>\n",
              "      <td>0.996369</td>\n",
              "      <td>0.988796</td>\n",
              "      <td>0.977604</td>\n",
              "      <td>0.988194</td>\n",
              "      <td>0.977560</td>\n",
              "      <td>0.997616</td>\n",
              "      <td>0.994294</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.261065</td>\n",
              "      <td>RandomForest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dim Reduction</th>\n",
              "      <td>0.988081</td>\n",
              "      <td>0.983700</td>\n",
              "      <td>0.997821</td>\n",
              "      <td>0.990330</td>\n",
              "      <td>0.981326</td>\n",
              "      <td>0.974244</td>\n",
              "      <td>0.978749</td>\n",
              "      <td>0.973352</td>\n",
              "      <td>0.997593</td>\n",
              "      <td>0.994269</td>\n",
              "      <td>0.877098</td>\n",
              "      <td>0.265567</td>\n",
              "      <td>GradientBoosting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dim Reduction</th>\n",
              "      <td>0.985731</td>\n",
              "      <td>0.979505</td>\n",
              "      <td>0.995046</td>\n",
              "      <td>0.979640</td>\n",
              "      <td>0.977591</td>\n",
              "      <td>0.967525</td>\n",
              "      <td>0.978749</td>\n",
              "      <td>0.971950</td>\n",
              "      <td>0.992814</td>\n",
              "      <td>0.987179</td>\n",
              "      <td>0.807729</td>\n",
              "      <td>0.143178</td>\n",
              "      <td>LogisticRegression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dim Reduction</th>\n",
              "      <td>0.989286</td>\n",
              "      <td>0.983723</td>\n",
              "      <td>0.997847</td>\n",
              "      <td>0.991565</td>\n",
              "      <td>0.983193</td>\n",
              "      <td>0.974244</td>\n",
              "      <td>0.981110</td>\n",
              "      <td>0.974755</td>\n",
              "      <td>0.997599</td>\n",
              "      <td>0.992857</td>\n",
              "      <td>0.809253</td>\n",
              "      <td>0.297886</td>\n",
              "      <td>XGBoost</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Clustering</th>\n",
              "      <td>0.996466</td>\n",
              "      <td>0.983769</td>\n",
              "      <td>0.998763</td>\n",
              "      <td>0.990338</td>\n",
              "      <td>0.994413</td>\n",
              "      <td>0.974244</td>\n",
              "      <td>0.992958</td>\n",
              "      <td>0.977560</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.990057</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.172153</td>\n",
              "      <td>RandomForest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Clustering</th>\n",
              "      <td>0.996466</td>\n",
              "      <td>0.980892</td>\n",
              "      <td>0.999334</td>\n",
              "      <td>0.984716</td>\n",
              "      <td>0.994413</td>\n",
              "      <td>0.969765</td>\n",
              "      <td>0.992958</td>\n",
              "      <td>0.971950</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.935731</td>\n",
              "      <td>0.155137</td>\n",
              "      <td>GradientBoosting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Clustering</th>\n",
              "      <td>0.989362</td>\n",
              "      <td>0.976711</td>\n",
              "      <td>0.997859</td>\n",
              "      <td>0.983797</td>\n",
              "      <td>0.983240</td>\n",
              "      <td>0.963046</td>\n",
              "      <td>0.982394</td>\n",
              "      <td>0.970547</td>\n",
              "      <td>0.996429</td>\n",
              "      <td>0.982955</td>\n",
              "      <td>0.814117</td>\n",
              "      <td>0.069192</td>\n",
              "      <td>LogisticRegression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Clustering</th>\n",
              "      <td>0.980322</td>\n",
              "      <td>0.975852</td>\n",
              "      <td>0.998239</td>\n",
              "      <td>0.974883</td>\n",
              "      <td>0.969274</td>\n",
              "      <td>0.961926</td>\n",
              "      <td>0.964789</td>\n",
              "      <td>0.963534</td>\n",
              "      <td>0.996364</td>\n",
              "      <td>0.988489</td>\n",
              "      <td>0.938267</td>\n",
              "      <td>0.186601</td>\n",
              "      <td>XGBoost</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Anomaly Detection</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.962319</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.984362</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.941769</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.931276</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.995502</td>\n",
              "      <td>0.430000</td>\n",
              "      <td>0.256536</td>\n",
              "      <td>RandomForest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Anomaly Detection</th>\n",
              "      <td>0.996673</td>\n",
              "      <td>0.957091</td>\n",
              "      <td>0.999834</td>\n",
              "      <td>0.958326</td>\n",
              "      <td>0.995331</td>\n",
              "      <td>0.933931</td>\n",
              "      <td>0.998667</td>\n",
              "      <td>0.922861</td>\n",
              "      <td>0.994688</td>\n",
              "      <td>0.993958</td>\n",
              "      <td>0.485456</td>\n",
              "      <td>0.271453</td>\n",
              "      <td>GradientBoosting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Anomaly Detection</th>\n",
              "      <td>0.986649</td>\n",
              "      <td>0.953890</td>\n",
              "      <td>0.998700</td>\n",
              "      <td>0.946922</td>\n",
              "      <td>0.981326</td>\n",
              "      <td>0.928331</td>\n",
              "      <td>0.985333</td>\n",
              "      <td>0.928471</td>\n",
              "      <td>0.987968</td>\n",
              "      <td>0.980741</td>\n",
              "      <td>0.408349</td>\n",
              "      <td>0.142090</td>\n",
              "      <td>LogisticRegression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Anomaly Detection</th>\n",
              "      <td>0.997996</td>\n",
              "      <td>0.957787</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.962416</td>\n",
              "      <td>0.997199</td>\n",
              "      <td>0.935050</td>\n",
              "      <td>0.996000</td>\n",
              "      <td>0.922861</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.995461</td>\n",
              "      <td>0.818739</td>\n",
              "      <td>0.231161</td>\n",
              "      <td>XGBoost</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Entropy</th>\n",
              "      <td>0.994633</td>\n",
              "      <td>0.983927</td>\n",
              "      <td>0.999708</td>\n",
              "      <td>0.978370</td>\n",
              "      <td>0.994398</td>\n",
              "      <td>0.974244</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.987377</td>\n",
              "      <td>0.989324</td>\n",
              "      <td>0.980501</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.250904</td>\n",
              "      <td>RandomForest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Entropy</th>\n",
              "      <td>0.941476</td>\n",
              "      <td>0.972822</td>\n",
              "      <td>0.996782</td>\n",
              "      <td>0.967734</td>\n",
              "      <td>0.935574</td>\n",
              "      <td>0.956327</td>\n",
              "      <td>0.998201</td>\n",
              "      <td>0.978962</td>\n",
              "      <td>0.890851</td>\n",
              "      <td>0.966759</td>\n",
              "      <td>0.116102</td>\n",
              "      <td>0.276588</td>\n",
              "      <td>GradientBoosting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Entropy</th>\n",
              "      <td>0.922034</td>\n",
              "      <td>0.955056</td>\n",
              "      <td>0.987084</td>\n",
              "      <td>0.937588</td>\n",
              "      <td>0.914099</td>\n",
              "      <td>0.928331</td>\n",
              "      <td>0.978417</td>\n",
              "      <td>0.953717</td>\n",
              "      <td>0.871795</td>\n",
              "      <td>0.956399</td>\n",
              "      <td>0.199447</td>\n",
              "      <td>0.142381</td>\n",
              "      <td>LogisticRegression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Entropy</th>\n",
              "      <td>0.934454</td>\n",
              "      <td>0.972145</td>\n",
              "      <td>0.999778</td>\n",
              "      <td>0.965498</td>\n",
              "      <td>0.927171</td>\n",
              "      <td>0.955207</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.978962</td>\n",
              "      <td>0.876972</td>\n",
              "      <td>0.965422</td>\n",
              "      <td>0.019008</td>\n",
              "      <td>0.243313</td>\n",
              "      <td>XGBoost</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Distance Based</th>\n",
              "      <td>0.991140</td>\n",
              "      <td>0.980756</td>\n",
              "      <td>0.999531</td>\n",
              "      <td>0.996295</td>\n",
              "      <td>0.985994</td>\n",
              "      <td>0.969765</td>\n",
              "      <td>0.982436</td>\n",
              "      <td>0.964937</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.997101</td>\n",
              "      <td>0.730000</td>\n",
              "      <td>0.247783</td>\n",
              "      <td>RandomForest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Distance Based</th>\n",
              "      <td>0.990544</td>\n",
              "      <td>0.982232</td>\n",
              "      <td>0.999277</td>\n",
              "      <td>0.990393</td>\n",
              "      <td>0.985061</td>\n",
              "      <td>0.972004</td>\n",
              "      <td>0.981265</td>\n",
              "      <td>0.969144</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.995677</td>\n",
              "      <td>0.887572</td>\n",
              "      <td>0.265048</td>\n",
              "      <td>GradientBoosting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Distance Based</th>\n",
              "      <td>0.989399</td>\n",
              "      <td>0.980254</td>\n",
              "      <td>0.997372</td>\n",
              "      <td>0.980665</td>\n",
              "      <td>0.983193</td>\n",
              "      <td>0.968645</td>\n",
              "      <td>0.983607</td>\n",
              "      <td>0.974755</td>\n",
              "      <td>0.995261</td>\n",
              "      <td>0.985816</td>\n",
              "      <td>0.645286</td>\n",
              "      <td>0.141880</td>\n",
              "      <td>LogisticRegression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Distance Based</th>\n",
              "      <td>0.989362</td>\n",
              "      <td>0.981534</td>\n",
              "      <td>0.999325</td>\n",
              "      <td>0.992485</td>\n",
              "      <td>0.983193</td>\n",
              "      <td>0.970885</td>\n",
              "      <td>0.980094</td>\n",
              "      <td>0.969144</td>\n",
              "      <td>0.998807</td>\n",
              "      <td>0.994245</td>\n",
              "      <td>0.796098</td>\n",
              "      <td>0.229198</td>\n",
              "      <td>XGBoost</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-73784d8f-7e93-47f1-86ea-cf5a0aabc5ce')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-73784d8f-7e93-47f1-86ea-cf5a0aabc5ce button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-73784d8f-7e93-47f1-86ea-cf5a0aabc5ce');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wYo9W66fmEE0"
      },
      "execution_count": 16,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN3kPRX2UW3rx4vMwHEjdzh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}