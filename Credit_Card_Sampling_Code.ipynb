{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vibhuverma17/MLBASEDSAMPLING/blob/main/Credit_Card_Sampling_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aad7i3jF5Rcf",
        "outputId": "d50f2b66-d1c9-4fd7-d438-fddd40374fab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.10/dist-packages (0.5.7)\n",
            "Requirement already satisfied: hdbscan in /usr/local/lib/python3.10/dist-packages (0.8.40)\n",
            "Requirement already satisfied: gower in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: kmodes in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: XGBoost in /usr/local/lib/python3.10/dist-packages (2.1.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.5.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.5.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.6)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan) (1.4.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from XGBoost) (2.23.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install umap-learn hdbscan gower kmodes XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ov6EwiM05WKy"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# Standard Libraries\n",
        "# ===============================\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import ast\n",
        "\n",
        "# ===============================\n",
        "# Visualization Libraries\n",
        "# ===============================\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "\n",
        "# ===============================\n",
        "# Scikit-learn Components\n",
        "# ===============================\n",
        "# Data Splitting and Preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Models\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, RandomForestRegressor,\n",
        "    GradientBoostingClassifier, GradientBoostingRegressor,\n",
        "    IsolationForest\n",
        ")\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import (\n",
        "    f1_score, roc_auc_score, accuracy_score, recall_score, precision_score,\n",
        "    mean_absolute_error, mean_squared_error, r2_score,\n",
        "    mean_absolute_percentage_error, roc_curve\n",
        ")\n",
        "\n",
        "# Pairwise distances\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "\n",
        "# ===============================\n",
        "# External Libraries\n",
        "# ===============================\n",
        "import xgboost as xgb  # XGBoost library\n",
        "from scipy.stats import ks_2samp, entropy  # Statistical tests\n",
        "from scipy.spatial.distance import cdist\n",
        "from kmodes.kprototypes import KPrototypes  # Clustering\n",
        "import umap  # Dimensionality reduction\n",
        "import hdbscan  # Density-based clustering\n",
        "import gower  # Gower similarity for mixed data types\n",
        "\n",
        "# ===============================\n",
        "# Configure Warnings\n",
        "# ===============================\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBV8SEqcwY-L"
      },
      "source": [
        "#### DIM REDUCTION AND GRID SEARCH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XQsa-dNvH2vc"
      },
      "outputs": [],
      "source": [
        "class StratifiedSamplingUMAP:\n",
        "    def __init__(self, n_neighbors=15, min_dist=0.1, n_components=2, n_grids=10, sample_percentage=0.1):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.min_dist = min_dist\n",
        "        self.n_components = n_components\n",
        "        self.n_grids = n_grids\n",
        "        self.sample_percentage = sample_percentage\n",
        "        self.sample_indices = None  # To store sampled indices\n",
        "\n",
        "    def check_categorical(self, data):\n",
        "        return data.select_dtypes(include=['object']).shape[1] > 0\n",
        "\n",
        "    def fit_transform(self, data):\n",
        "        if self.check_categorical(data):\n",
        "            print(\"Categorical variables detected. Using 'dice' metric for UMAP.\")\n",
        "            categorical_data = data.select_dtypes(include=['object'])\n",
        "            encoder = OneHotEncoder(sparse_output=False)\n",
        "            categorical_encoded = encoder.fit_transform(categorical_data)\n",
        "            continuous_data = data.select_dtypes(exclude=['object'])\n",
        "            combined_data = np.hstack([categorical_encoded, continuous_data])\n",
        "            metric = 'dice'\n",
        "        else:\n",
        "            print(\"No categorical variables detected. Using 'euclidean' metric for UMAP.\")\n",
        "            combined_data = data\n",
        "            metric = 'euclidean'\n",
        "\n",
        "        umap_model = umap.UMAP(n_neighbors=self.n_neighbors, min_dist=self.min_dist,\n",
        "                               n_components=self.n_components, metric=metric)\n",
        "        return umap_model.fit_transform(combined_data)\n",
        "\n",
        "    def stratified_sampling(self, embedding):\n",
        "        min_x, min_y = np.min(embedding[:, :2], axis=0)\n",
        "        max_x, max_y = np.max(embedding[:, :2], axis=0)\n",
        "\n",
        "        x_bins = np.linspace(min_x, max_x, self.n_grids)\n",
        "        y_bins = np.linspace(min_y, max_y, self.n_grids)\n",
        "\n",
        "        if self.n_components == 3:\n",
        "            min_z = np.min(embedding[:, 2])\n",
        "            max_z = np.max(embedding[:, 2])\n",
        "            z_bins = np.linspace(min_z, max_z, self.n_grids)\n",
        "            grid_counts = np.zeros((self.n_grids, self.n_grids, self.n_grids))\n",
        "        else:\n",
        "            grid_counts = np.zeros((self.n_grids, self.n_grids))\n",
        "\n",
        "        for point in embedding:\n",
        "            x_idx = np.digitize(point[0], x_bins) - 1\n",
        "            y_idx = np.digitize(point[1], y_bins) - 1\n",
        "\n",
        "            if self.n_components == 3:\n",
        "                z_idx = np.digitize(point[2], z_bins) - 1\n",
        "                grid_counts[x_idx, y_idx, z_idx] += 1\n",
        "            else:\n",
        "                grid_counts[x_idx, y_idx] += 1\n",
        "\n",
        "        grid_probs = grid_counts / np.sum(grid_counts)\n",
        "        grid_probs_flat = grid_probs.flatten()\n",
        "\n",
        "        # Sample indices based on the probability distribution\n",
        "        sampled_indices = np.random.choice(len(embedding), size=int(len(embedding) * self.sample_percentage), replace=False)\n",
        "\n",
        "        # Store the sampled indices for later retrieval\n",
        "        self.sample_indices = sampled_indices\n",
        "\n",
        "        # Extract the corresponding samples from the embedding\n",
        "        sampled_embedding = embedding[sampled_indices]\n",
        "\n",
        "        return sampled_embedding\n",
        "\n",
        "    def get_sample_indices(self):\n",
        "        \"\"\"\n",
        "        Return the indices of the sampled data points in the original dataset.\n",
        "        \"\"\"\n",
        "        if self.sample_indices is None:\n",
        "            raise ValueError(\"No samples have been selected. Please run stratified_sampling first.\")\n",
        "        return self.sample_indices\n",
        "\n",
        "    def plot(self, embedding):\n",
        "        if self.n_components == 2:\n",
        "            plt.scatter(embedding[:, 0], embedding[:, 1], c='blue', marker='o')\n",
        "            plt.title('UMAP Projection (2D)')\n",
        "            plt.xlabel('UMAP 1')\n",
        "            plt.ylabel('UMAP 2')\n",
        "            plt.show()\n",
        "\n",
        "        elif self.n_components == 3:\n",
        "            fig = plt.figure()\n",
        "            ax = fig.add_subplot(111, projection='3d')\n",
        "            ax.scatter(embedding[:, 0], embedding[:, 1], embedding[:, 2], c='blue', marker='o')\n",
        "            ax.set_title('UMAP Projection (3D)')\n",
        "            ax.set_xlabel('UMAP 1')\n",
        "            ax.set_ylabel('UMAP 2')\n",
        "            ax.set_zlabel('UMAP 3')\n",
        "            plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eao-WLtwwS_7"
      },
      "source": [
        "##### HDBSCAN - CLUSTERING BASED SAMPLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "emjvmMeDMmhj"
      },
      "outputs": [],
      "source": [
        "class ClusterSampler:\n",
        "    def __init__(self, data, sampling_percent=10, **hdbscan_params):\n",
        "        \"\"\"\n",
        "        Initialize the ClusterSampler class.\n",
        "\n",
        "        Parameters:\n",
        "        - data: DataFrame containing the dataset\n",
        "        - sampling_percent: Percentage of points to sample from each cluster (0-100)\n",
        "        - **hdbscan_params: Additional parameters to pass to HDBSCAN\n",
        "        \"\"\"\n",
        "        self.data = self._convert_data_types(data)\n",
        "        self.sampling_percent = sampling_percent\n",
        "        self.is_categorical = self._detect_categorical(data)\n",
        "        self.cluster_labels = None\n",
        "        self.hdbscan_params = hdbscan_params\n",
        "\n",
        "    def _convert_data_types(self, data):\n",
        "        \"\"\"Ensure continuous columns are float64 and categorical columns are object.\"\"\"\n",
        "        continuous_cols = data.select_dtypes(include=['float', 'int']).columns\n",
        "        data[continuous_cols] = data[continuous_cols].astype(np.float64)\n",
        "\n",
        "        categorical_cols = data.select_dtypes(include=['object', 'category']).columns\n",
        "        data[categorical_cols] = data[categorical_cols].astype('object')\n",
        "\n",
        "        return data\n",
        "\n",
        "    def _detect_categorical(self, data):\n",
        "        \"\"\"Detect if the dataset contains categorical features.\"\"\"\n",
        "        return data.select_dtypes(include=['object', 'category']).shape[1] > 0\n",
        "\n",
        "    def _compute_distance_matrix(self):\n",
        "        \"\"\"Compute the distance matrix based on the data type.\"\"\"\n",
        "        if self.is_categorical:\n",
        "            gower_matrix = gower.gower_matrix(self.data)\n",
        "            return gower_matrix.astype(np.float64)\n",
        "        else:\n",
        "            return pairwise_distances(self.data, metric='euclidean')\n",
        "\n",
        "    def fit_clusters(self):\n",
        "        \"\"\"Fit HDBSCAN on the dataset with appropriate distance metric.\"\"\"\n",
        "        distance_matrix = self._compute_distance_matrix()\n",
        "        clusterer = hdbscan.HDBSCAN(metric='precomputed' if self.is_categorical else 'euclidean', **self.hdbscan_params)\n",
        "        self.cluster_labels = clusterer.fit_predict(distance_matrix)\n",
        "\n",
        "    def sample_points(self):\n",
        "        \"\"\"Sample a representative subset of points from each cluster, including noise points as a separate cluster.\"\"\"\n",
        "        if self.cluster_labels is None:\n",
        "            raise ValueError(\"Clusters have not been computed. Call fit_clusters() first.\")\n",
        "\n",
        "        data_with_labels = self.data.copy()\n",
        "        data_with_labels['cluster'] = self.cluster_labels\n",
        "\n",
        "        sampled_indices = []\n",
        "        unique_labels = np.unique(self.cluster_labels)\n",
        "\n",
        "        for cluster_label in unique_labels:\n",
        "            cluster_indices = data_with_labels[data_with_labels['cluster'] == cluster_label].index\n",
        "            sample_size = max(1, int(len(cluster_indices) * (self.sampling_percent / 100)))\n",
        "\n",
        "            # Avoid sampling more points than available\n",
        "            if len(cluster_indices) < sample_size:\n",
        "                print(f\"Cluster {cluster_label} has only {len(cluster_indices)} points, sampling {len(cluster_indices)}.\")\n",
        "            sampled_indices.extend(np.random.choice(cluster_indices, min(sample_size, len(cluster_indices)), replace=False))\n",
        "\n",
        "        # Ensure sampled indices are unique and valid\n",
        "        sampled_indices = list(set(sampled_indices))\n",
        "        sampled_indices = [idx for idx in sampled_indices if idx < len(self.data)]\n",
        "\n",
        "        return sampled_indices\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Execute the full sampling pipeline: cluster, then sample.\"\"\"\n",
        "        self.fit_clusters()\n",
        "        return self.sample_points()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGWSKfBWxJNc"
      },
      "source": [
        "##### ISOLATION FOREST AND KS STATISTIC SAMPLER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "t4BSBb9W-9kc"
      },
      "outputs": [],
      "source": [
        "class AnomalySampler:\n",
        "    def __init__(self, X, sample_weight=None):\n",
        "        \"\"\"\n",
        "        Initialize the AnomalySampler class.\n",
        "\n",
        "        Parameters:\n",
        "        - X: DataFrame containing the dataset\n",
        "        - sample_weight: Optional sample weights for the Isolation Forest\n",
        "        \"\"\"\n",
        "        self.original_data = X\n",
        "        self.X = self._one_hot_encode(X)\n",
        "        self.sample_weight = sample_weight\n",
        "        self.preds = self.isolation_forest(self.X, sample_weight)\n",
        "\n",
        "    def _one_hot_encode(self, data):\n",
        "        \"\"\"\n",
        "        Apply one-hot encoding to categorical columns.\n",
        "\n",
        "        Parameters:\n",
        "        - data: DataFrame containing the dataset\n",
        "\n",
        "        Returns:\n",
        "        - One-hot encoded DataFrame\n",
        "        \"\"\"\n",
        "        return pd.get_dummies(data, drop_first=True)  # drop_first to avoid multicollinearity, if relevant\n",
        "\n",
        "    @staticmethod\n",
        "    def isolation_forest(X, sample_weight=None):\n",
        "        \"\"\"\n",
        "        Fits an Isolation Forest to the dataset and assigns an anomaly score to each sample.\n",
        "\n",
        "        Parameters:\n",
        "        - X: DataFrame or array-like containing the dataset\n",
        "        - sample_weight: Optional sample weights for the Isolation Forest\n",
        "\n",
        "        Returns:\n",
        "        - preds: Anomaly scores for each sample\n",
        "        \"\"\"\n",
        "        clf = IsolationForest().fit(X, sample_weight=sample_weight)\n",
        "        preds = clf.score_samples(X)\n",
        "        return preds\n",
        "\n",
        "    @staticmethod\n",
        "    def get_5_percent(num):\n",
        "        \"\"\"Calculate 5% of a given number.\"\"\"\n",
        "        return round(5 / 100 * num)\n",
        "\n",
        "    def get_5_percent_splits(self, length):\n",
        "        \"\"\"Splits a given length into 5% intervals.\"\"\"\n",
        "        five_percent = self.get_5_percent(length)\n",
        "        return np.arange(five_percent, length, five_percent)\n",
        "\n",
        "    def find_sample_indices(self):\n",
        "        \"\"\"\n",
        "        Finds a sample by comparing the distribution of anomaly scores between the sample\n",
        "        and the original distribution using the KS-test. Starts with a 5% sample, increasing\n",
        "        by 5% increments until a significant sample (p-value > 0.95) is found or a limit is reached.\n",
        "\n",
        "        Returns:\n",
        "        - List of indices representing the sample in the original dataset\n",
        "        \"\"\"\n",
        "        size_splits = self.get_5_percent_splits(len(self.X))\n",
        "        element = 1\n",
        "        iteration = 0\n",
        "\n",
        "        while element < len(size_splits):\n",
        "            sample_size = size_splits[element]\n",
        "            sample_indices = np.random.choice(np.arange(self.preds.size), size=sample_size, replace=False)\n",
        "            sample = np.take(self.preds, sample_indices)\n",
        "\n",
        "            # Check if KS test p-value indicates similar distributions\n",
        "            if ks_2samp(self.preds, sample).pvalue > 0.95:\n",
        "                return sample_indices  # Return indices from the original dataset\n",
        "\n",
        "            iteration += 1\n",
        "            if iteration >= 20:\n",
        "                element += 1\n",
        "                iteration = 0\n",
        "\n",
        "        # If no suitable sample is found, return the last attempted sample indices\n",
        "        return sample_indices\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRaDp-XhxRho"
      },
      "source": [
        "#### ENTROPY SAMPLER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3a80f3-ir6gR"
      },
      "outputs": [],
      "source": [
        "class EntropySampler:\n",
        "    def __init__(self, data, sampling_percent=10, bins=10):\n",
        "        \"\"\"\n",
        "        Initialize the EntropySampler class.\n",
        "\n",
        "        Parameters:\n",
        "        - data: DataFrame containing the dataset\n",
        "        - sampling_percent: Percentage of points to sample based on entropy (0-100)\n",
        "        - bins: Number of bins to use for continuous data entropy calculation\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.sampling_percent = sampling_percent\n",
        "        self.bins = bins\n",
        "        self.entropy_scores = None\n",
        "\n",
        "    def _calculate_entropy(self):\n",
        "        \"\"\"\n",
        "        Calculate the entropy for each feature and aggregate entropy per data point.\n",
        "\n",
        "        Returns:\n",
        "        - entropy_scores: Array of entropy scores for each data point\n",
        "        \"\"\"\n",
        "        entropy_scores = np.zeros(len(self.data))\n",
        "\n",
        "        for col in self.data.columns:\n",
        "            if pd.api.types.is_numeric_dtype(self.data[col]):\n",
        "                # For continuous data, bin it and calculate entropy over the bins\n",
        "                counts, _ = np.histogram(self.data[col], bins=self.bins)\n",
        "                feature_entropy = entropy(counts + 1e-10)  # Add small value to avoid log(0)\n",
        "                feature_contributions = np.digitize(self.data[col], bins=np.histogram_bin_edges(self.data[col], bins=self.bins))\n",
        "            else:\n",
        "                # For categorical data, calculate entropy over unique values\n",
        "                counts = self.data[col].value_counts().values\n",
        "                feature_entropy = entropy(counts + 1e-10)\n",
        "                feature_contributions = self.data[col].map(self.data[col].value_counts(normalize=True)).values\n",
        "\n",
        "            # Accumulate entropy scores based on the feature contributions for each data point\n",
        "            entropy_scores += feature_contributions * feature_entropy\n",
        "\n",
        "        self.entropy_scores = entropy_scores\n",
        "\n",
        "    def sample_indices(self):\n",
        "        \"\"\"\n",
        "        Get indices of points with the highest entropy scores.\n",
        "\n",
        "        Returns:\n",
        "        - List of indices for sampled points based on entropy scores\n",
        "        \"\"\"\n",
        "        if self.entropy_scores is None:\n",
        "            self._calculate_entropy()\n",
        "\n",
        "        # Determine the number of points to sample\n",
        "        num_samples = max(1, int(len(self.data) * (self.sampling_percent / 100)))\n",
        "\n",
        "        # Get the indices of the top entropy scores\n",
        "        top_indices = np.argsort(self.entropy_scores)[-num_samples:]\n",
        "\n",
        "        return top_indices\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Execute the full entropy-based sampling process.\n",
        "\n",
        "        Returns:\n",
        "        - List of indices for sampled points based on entropy scores\n",
        "        \"\"\"\n",
        "        self._calculate_entropy()\n",
        "        return self.sample_indices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8XnIZ2O33UO"
      },
      "source": [
        "### DISTANCE BASED SAMPLER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1qgqGwih34r9"
      },
      "outputs": [],
      "source": [
        "class DistanceBasedSampler:\n",
        "    def __init__(self, data, k=5, sampling_percent=10):\n",
        "        self.data = data.reset_index(drop=True)  # Reset index for consistent access\n",
        "        self.k = k\n",
        "        self.sampling_percent = sampling_percent\n",
        "        self.cluster_labels = None\n",
        "        self.cluster_centroids = None\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        \"\"\"Ensure all categorical columns are treated as strings.\"\"\"\n",
        "        self.data = self.data.apply(lambda col: col.astype(str) if col.dtype == 'object' else col)\n",
        "        return self.data\n",
        "\n",
        "    def fit_clusters(self):\n",
        "        \"\"\"\n",
        "        Fit clusters using K-Prototypes if categorical data is present,\n",
        "        otherwise fallback to K-Means for purely numerical data.\n",
        "        \"\"\"\n",
        "        self._prepare_data()\n",
        "        data_array = self.data.values\n",
        "\n",
        "        # Identify categorical column indices\n",
        "        categorical_indices = [\n",
        "            i for i, col in enumerate(self.data.columns) if self.data.dtypes[col] == 'object'\n",
        "        ]\n",
        "\n",
        "        if len(categorical_indices) == 0:\n",
        "            # No categorical data, use K-Means\n",
        "            print(\"No categorical data detected. Using K-Means instead.\")\n",
        "            kmeans = KMeans(n_clusters=self.k, random_state=42)\n",
        "            clusters = kmeans.fit_predict(data_array)\n",
        "            self.cluster_labels = clusters\n",
        "            self.cluster_centroids = kmeans.cluster_centers_\n",
        "        else:\n",
        "            # Use K-Prototypes for mixed data\n",
        "            kproto = KPrototypes(n_clusters=self.k, init='Cao', random_state=42)\n",
        "            clusters = kproto.fit_predict(data_array, categorical=categorical_indices)\n",
        "            self.cluster_labels = clusters\n",
        "            self.cluster_centroids = kproto.cluster_centroids_\n",
        "\n",
        "        return clusters\n",
        "\n",
        "    def _calculate_mixed_distance(self, point, centroids):\n",
        "        \"\"\"\n",
        "        Calculate the mixed distance (numerical + categorical) from a point to centroids.\n",
        "        \"\"\"\n",
        "        numeric_cols = self.data.select_dtypes(include=[np.number]).columns\n",
        "        categorical_cols = self.data.select_dtypes(include=['object']).columns\n",
        "\n",
        "        point_numeric = point[numeric_cols].values.astype(float)\n",
        "        point_categorical = point[categorical_cols].values\n",
        "\n",
        "        distances = []\n",
        "        for centroid in centroids:\n",
        "            # Split centroids into numeric and categorical parts\n",
        "            centroid_numeric = np.array(centroid[:len(numeric_cols)], dtype=float)\n",
        "            centroid_categorical = np.array(centroid[len(numeric_cols):])\n",
        "\n",
        "            # Calculate numerical distance (Euclidean)\n",
        "            numeric_distance = np.linalg.norm(point_numeric - centroid_numeric)\n",
        "            # Calculate categorical distance (Hamming)\n",
        "            categorical_distance = np.sum(point_categorical != centroid_categorical)\n",
        "\n",
        "            distances.append(numeric_distance + categorical_distance)\n",
        "\n",
        "        return np.array(distances)\n",
        "\n",
        "    def sample_indices(self):\n",
        "        \"\"\"\n",
        "        Identify sample indices based on distance to centroids.\n",
        "        \"\"\"\n",
        "        if self.cluster_labels is None:\n",
        "            self.fit_clusters()\n",
        "\n",
        "        centroids = self.cluster_centroids\n",
        "        distances = np.zeros(len(self.data))\n",
        "\n",
        "        for i, (_, point) in enumerate(self.data.iterrows()):\n",
        "            distances[i] = np.min(self._calculate_mixed_distance(point, centroids))\n",
        "\n",
        "        num_samples = max(1, int(len(self.data) * (self.sampling_percent / 100)))\n",
        "        sampled_indices = np.argsort(distances)[-num_samples:]\n",
        "\n",
        "        return sampled_indices\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Run the sampling process.\n",
        "        \"\"\"\n",
        "        self.fit_clusters()\n",
        "        return self.sample_indices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5w6E43q4zYH"
      },
      "source": [
        "#### RANDOM SAMPLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lZclmsfb4vgc"
      },
      "outputs": [],
      "source": [
        "class RandomSampler:\n",
        "    def __init__(self, data, sampling_percent=10):\n",
        "        \"\"\"\n",
        "        Initialize the RandomSampler class.\n",
        "\n",
        "        Parameters:\n",
        "        - data: DataFrame containing the dataset\n",
        "        - sampling_percent: Percentage of rows to sample (0-100)\n",
        "        \"\"\"\n",
        "        self.data = data.reset_index(drop=True)  # Reset index to ensure consistency\n",
        "        self.sampling_percent = sampling_percent\n",
        "\n",
        "    def sample_indices(self):\n",
        "        \"\"\"\n",
        "        Randomly sample indices of rows from the dataset.\n",
        "\n",
        "        Returns:\n",
        "        - A list of sampled indices.\n",
        "        \"\"\"\n",
        "        if not (0 <= self.sampling_percent <= 100):\n",
        "            raise ValueError(\"sampling_percent must be between 0 and 100.\")\n",
        "\n",
        "        # Calculate the number of samples to draw\n",
        "        sample_size = int(len(self.data) * (self.sampling_percent / 100))\n",
        "        sample_size = max(1, sample_size)  # Ensure at least one index is selected\n",
        "\n",
        "        # Perform random sampling without replacement and return indices\n",
        "        sampled_indices = self.data.sample(n=sample_size, random_state=42).index.tolist()\n",
        "        return sampled_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C74qxcZZ42Rz"
      },
      "source": [
        "### MACHINE LEARNING MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9M0kBWKKsi6N"
      },
      "outputs": [],
      "source": [
        "# !mkdir Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuZdfS3kN2OZ"
      },
      "source": [
        "#### READING DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VNwWlcPn-eqD"
      },
      "outputs": [],
      "source": [
        "# Custom transformer to drop highly correlated features\n",
        "class DropHighCorrelation(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, threshold=0.6):\n",
        "        \"\"\"\n",
        "        Custom transformer to drop highly correlated features.\n",
        "\n",
        "        Parameters:\n",
        "        - threshold: Correlation threshold above which features will be dropped.\n",
        "        \"\"\"\n",
        "        self.threshold = threshold\n",
        "        self.to_drop = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Identify features to drop based on the correlation threshold.\n",
        "        \"\"\"\n",
        "        corr_matrix = pd.DataFrame(X).corr().abs()  # Compute the absolute correlation matrix\n",
        "        upper_tri = corr_matrix.where(\n",
        "            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
        "        )  # Extract upper triangle\n",
        "        self.to_drop = [\n",
        "            column for column in upper_tri.columns if any(upper_tri[column] > self.threshold)\n",
        "        ]\n",
        "\n",
        "        print(f\"DropHighCorrelation: {len(self.to_drop)} columns will be dropped due to correlation (threshold={self.threshold}).\")\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Drop the identified features from the dataset.\n",
        "        \"\"\"\n",
        "        return pd.DataFrame(X).drop(columns=self.to_drop, errors='ignore')\n",
        "\n",
        "# Main ModelTrainer class\n",
        "class ModelTrainer:\n",
        "    def __init__(self, datasets_X, datasets_y, task_type, dataset_names=None):\n",
        "        \"\"\"\n",
        "        Initializes the ModelTrainer with datasets, explicitly provided task type, and dataset names.\n",
        "\n",
        "        Parameters:\n",
        "        - datasets_X: List of [X_train, X_test] for different datasets\n",
        "        - datasets_y: List of [y_train, y_test] for different datasets\n",
        "        - task_type: A string explicitly specifying the task type, either 'regression' or 'classification'.\n",
        "        - dataset_names: List of names for the datasets to be used as index in the results DataFrame\n",
        "        \"\"\"\n",
        "        if task_type not in ['classification', 'regression']:\n",
        "            raise ValueError(\"task_type must be either 'classification' or 'regression'\")\n",
        "\n",
        "        self.datasets_X = datasets_X\n",
        "        self.datasets_y = datasets_y\n",
        "        self.task_type = task_type\n",
        "        self.dataset_names = dataset_names\n",
        "\n",
        "    def _select_model(self):\n",
        "        \"\"\"Select model based on task type.\"\"\"\n",
        "        if self.task_type == 'classification':\n",
        "            return {\n",
        "                'RandomForest': RandomForestClassifier(random_state=42),\n",
        "                'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
        "                'LogisticRegression': LogisticRegression(),\n",
        "                'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "            }\n",
        "        elif self.task_type == 'regression':\n",
        "            return {\n",
        "                'RandomForest': RandomForestRegressor(random_state=42),\n",
        "                'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
        "                'LinearRegression': LinearRegression(),\n",
        "                'XGBoost': xgb.XGBRegressor(random_state=42)\n",
        "            }\n",
        "\n",
        "    def _create_pipeline(self, model, X):\n",
        "        \"\"\"Create a preprocessing and modeling pipeline.\"\"\"\n",
        "        # Identify categorical and numerical features\n",
        "        categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "        numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "        # Preprocessing for numerical features: Standard scaling\n",
        "        numerical_transformer = StandardScaler()\n",
        "\n",
        "        # Preprocessing for categorical features: One-hot encoding\n",
        "        categorical_transformer = OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False)\n",
        "\n",
        "        # Combine preprocessors in a column transformer\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numerical_transformer, numerical_features),\n",
        "                ('cat', categorical_transformer, categorical_features)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Define a pipeline with preprocessing, correlation dropping, and the specified model\n",
        "        pipeline = Pipeline(steps=[\n",
        "            ('preprocessor', preprocessor),  # First preprocess\n",
        "            ('drop_high_corr', DropHighCorrelation(threshold=0.6)),  # Then drop highly correlated features\n",
        "            ('model', model)  # Finally, apply the model\n",
        "        ])\n",
        "        return pipeline\n",
        "\n",
        "    def _get_best_cutoff(self, y_true, y_pred_proba):\n",
        "        \"\"\"Use Youden's J statistic to determine the best cutoff point for classification.\"\"\"\n",
        "        fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
        "        youden_index = tpr - fpr\n",
        "        best_cutoff = thresholds[np.argmax(youden_index)]\n",
        "        return best_cutoff\n",
        "\n",
        "    def _train_and_evaluate(self, model, X_train, X_test, y_train, y_test):\n",
        "        \"\"\"Train the model and evaluate it on both the training and test datasets.\"\"\"\n",
        "        start_time = time.time()\n",
        "        model.fit(X_train, y_train)\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        y_pred_train = model.predict(X_train)\n",
        "        y_pred_test = model.predict(X_test)\n",
        "\n",
        "        if self.task_type == 'classification':\n",
        "            y_pred_proba_test = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else y_pred_test\n",
        "            y_pred_proba_train = model.predict_proba(X_train)[:, 1] if hasattr(model, 'predict_proba') else y_pred_train\n",
        "\n",
        "            best_cutoff = self._get_best_cutoff(y_test, y_pred_proba_test)\n",
        "            y_pred_class_test = (y_pred_proba_test >= best_cutoff).astype(int)\n",
        "            y_pred_class_train = (y_pred_proba_train >= best_cutoff).astype(int)\n",
        "\n",
        "            metrics = {\n",
        "                'Train F1': f1_score(y_train, y_pred_class_train),\n",
        "                'Test F1': f1_score(y_test, y_pred_class_test),\n",
        "                'Train AUC': roc_auc_score(y_train, y_pred_proba_train),\n",
        "                'Test AUC': roc_auc_score(y_test, y_pred_proba_test),\n",
        "                'Train Accuracy': accuracy_score(y_train, y_pred_class_train),\n",
        "                'Test Accuracy': accuracy_score(y_test, y_pred_class_test),\n",
        "                'Train Recall': recall_score(y_train, y_pred_class_train),\n",
        "                'Test Recall': recall_score(y_test, y_pred_class_test),\n",
        "                'Train Precision': precision_score(y_train, y_pred_class_train),\n",
        "                'Test Precision': precision_score(y_test, y_pred_class_test),\n",
        "                'Best Cutoff': best_cutoff,\n",
        "                'Training Time (seconds)': training_time\n",
        "            }\n",
        "        else:\n",
        "            metrics = {\n",
        "                'Train MSE': mean_squared_error(y_train, y_pred_train),\n",
        "                'Test MSE': mean_squared_error(y_test, y_pred_test),\n",
        "                'Train MAPE': mean_absolute_percentage_error(y_train, y_pred_train),\n",
        "                'Test MAPE': mean_absolute_percentage_error(y_test, y_pred_test),\n",
        "                'Train R2': r2_score(y_train, y_pred_train),\n",
        "                'Test R2': r2_score(y_test, y_pred_test),\n",
        "                'Training Time (seconds)': training_time\n",
        "            }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def train_models(self):\n",
        "        \"\"\"Train and evaluate models on multiple datasets and return a DataFrame of results.\"\"\"\n",
        "        models = self._select_model()\n",
        "        results = []\n",
        "\n",
        "        for idx, (X_data, y_data) in enumerate(zip(self.datasets_X, self.datasets_y)):\n",
        "            X_train, X_test = X_data\n",
        "            y_train, y_test = y_data\n",
        "\n",
        "            for model_name, model in models.items():\n",
        "                pipeline = self._create_pipeline(model, X_train)\n",
        "                metrics = self._train_and_evaluate(pipeline, X_train, X_test, y_train, y_test)\n",
        "                metrics['Dataset'] = self.dataset_names[idx] if self.dataset_names else f'Dataset {idx+1}'\n",
        "                metrics['Model'] = model_name\n",
        "                results.append(metrics)\n",
        "\n",
        "        return pd.DataFrame(results).set_index('Dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "5m4zGB5rsjZU",
        "outputId": "122efbf1-7126-4cc7-f1ca-ffa840d9f2fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data:\n",
            "(284807, 31)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         V1        V2        V3        V4        V5        V6        V7  \\\n",
              "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
              "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
              "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
              "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
              "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
              "\n",
              "         V8        V9       V10  ...       V20       V21       V22       V23  \\\n",
              "0  0.098698  0.363787  0.090794  ...  0.251412 -0.018307  0.277838 -0.110474   \n",
              "1  0.085102 -0.255425 -0.166974  ... -0.069083 -0.225775 -0.638672  0.101288   \n",
              "2  0.247676 -1.514654  0.207643  ...  0.524980  0.247998  0.771679  0.909412   \n",
              "3  0.377436 -1.387024 -0.054952  ... -0.208038 -0.108300  0.005274 -0.190321   \n",
              "4 -0.270533  0.817739  0.753074  ...  0.408542 -0.009431  0.798278 -0.137458   \n",
              "\n",
              "        V24       V25       V26       V27       V28  Amount  \n",
              "0  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62  \n",
              "1 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69  \n",
              "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66  \n",
              "3 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50  \n",
              "4  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99  \n",
              "\n",
              "[5 rows x 29 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-94427873-ad8b-4270-a606-3d1bfad5cb84\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>...</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>...</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>...</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.208038</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>...</td>\n",
              "      <td>0.408542</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 29 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-94427873-ad8b-4270-a606-3d1bfad5cb84')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-94427873-ad8b-4270-a606-3d1bfad5cb84 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-94427873-ad8b-4270-a606-3d1bfad5cb84');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-45c36995-7ad1-478d-850d-217b79b0c9a3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-45c36995-7ad1-478d-850d-217b79b0c9a3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-45c36995-7ad1-478d-850d-217b79b0c9a3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "train = pd.read_csv('/content/Data/creditcard.csv')\n",
        "\n",
        "train.dropna()\n",
        "# Display the first few rows of the training data\n",
        "print(\"Training Data:\")\n",
        "# print(train.head())\n",
        "\n",
        "print(train.shape)\n",
        "\n",
        "train.drop(columns=['Time'],axis=1)\n",
        "# test.drop(columns=['id'],axis=1)\n",
        "\n",
        "# Split the data into features (X) and target (y\n",
        "X = train.drop(columns=['Time','Class'])\n",
        "y = train['Class']\n",
        "\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "r-r4zOJIwkmp"
      },
      "outputs": [],
      "source": [
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTMpo3HS4Drb",
        "outputId": "25f9ca5c-0113-443d-eae5-c35d102de2c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No categorical variables detected. Using 'euclidean' metric for UMAP.\n",
            "Sampled Data: 68353 Data: 227845\n",
            "Time taken for Dim Red and Grid Search Sampling: 395.9999420642853 seconds\n",
            "Sampled Data: 22784 Data: 227845\n",
            "Time taken for Isolation Forest and KS Sampling: 1.8184144496917725 seconds\n",
            "Sampled Data: 68353 Data: 227845\n",
            "Time taken for Entropy Sampling: 0.34904980659484863 seconds\n",
            "No categorical data detected. Using K-Means instead.\n",
            "Sampled Data: 68353 Data: 227845\n",
            "Time taken for Distance Based Sampling: 2904.457640647888 seconds\n",
            "Sampled Data: 68353 Data: 227845\n",
            "Time taken for Random Sampling: 0.06696867942810059 seconds\n"
          ]
        }
      ],
      "source": [
        "#### SAMPLING METHODS ####\n",
        "data = X_train1\n",
        "# ================================ #\n",
        "##### DIM REDUCTION AND GRID SEARCH\n",
        "# ================================ #\n",
        "start_time = time.time()\n",
        "stratified_sampler = StratifiedSamplingUMAP(n_neighbors=15, min_dist=0.1, n_components=3, n_grids=10, sample_percentage=0.3)\n",
        "embedding = stratified_sampler.fit_transform(data)\n",
        "sampled_embedding = stratified_sampler.stratified_sampling(embedding)\n",
        "sample_indices = stratified_sampler.get_sample_indices()\n",
        "print(\"Sampled Data:\", len(sample_indices),\"Data:\",len(data))\n",
        "X_train1_1 = data.iloc[sample_indices]\n",
        "y_train1_1 = y_train1.iloc[sample_indices]\n",
        "print(\"Time taken for Dim Red and Grid Search Sampling:\", time.time() - start_time, \"seconds\")\n",
        "\n",
        "# # ================================ #\n",
        "# ##### HDBSCAN - CLUSTERING BASED SAMPLING\n",
        "# # ================================ #\n",
        "\n",
        "# start_time = time.time()\n",
        "# sampler = ClusterSampler(data, sampling_percent=30, min_cluster_size=15, min_samples=3)\n",
        "# sampled_indices = sampler.run()\n",
        "# X_train1_2 = data.loc[sampled_indices]\n",
        "# y_train1_2 = y_train1.iloc[sampled_indices]\n",
        "# print(\"Sampled Data:\", len(sampled_indices),\"Data:\",len(data))\n",
        "# print(\"Time taken for Clustering Based Sampling:\", time.time() - start_time, \"seconds\")\n",
        "\n",
        "# ================================ #\n",
        "##### ISOLATION FOREST AND KS STATISTIC SAMPLER #####\n",
        "# ================================ #\n",
        "\n",
        "start_time = time.time()\n",
        "sampler = AnomalySampler(data)\n",
        "sample_indices = sampler.find_sample_indices()\n",
        "X_train1_3 = data.iloc[sample_indices]\n",
        "y_train1_3 = y_train1.iloc[sample_indices]\n",
        "print(\"Sampled Data:\", len(sample_indices),\"Data:\",len(data))\n",
        "print(\"Time taken for Isolation Forest and KS Sampling:\", time.time() - start_time, \"seconds\")\n",
        "\n",
        "# ================================ #\n",
        "##### ENTROPY SAMPLER #####\n",
        "# ================================ #\n",
        "\n",
        "start_time = time.time()\n",
        "sampler = EntropySampler(data,sampling_percent=30)\n",
        "sample_indices = sampler.run()\n",
        "X_train1_4 = data.iloc[sample_indices]\n",
        "y_train1_4 = y_train1.iloc[sample_indices]\n",
        "print(\"Sampled Data:\", len(sample_indices),\"Data:\",len(data))\n",
        "print(\"Time taken for Entropy Sampling:\", time.time() - start_time, \"seconds\")\n",
        "\n",
        "# ================================ #\n",
        "##### DISTANCE BASED SAMPLER #####\n",
        "# ================================ #\n",
        "\n",
        "start_time = time.time()\n",
        "sampler = DistanceBasedSampler(data, k=5, sampling_percent=30)\n",
        "sampled_indices = sampler.run()\n",
        "X_train1_5 = data.iloc[sampled_indices]\n",
        "y_train1_5 = y_train1.iloc[sampled_indices]\n",
        "print(\"Sampled Data:\", len(sampled_indices),\"Data:\",len(data))\n",
        "print(\"Time taken for Distance Based Sampling:\", time.time() - start_time, \"seconds\")\n",
        "\n",
        "# ================================ #\n",
        "##### RANDOM SAMPLER #####\n",
        "# ================================ #\n",
        "\n",
        "start_time = time.time()\n",
        "sampler = RandomSampler(data=data, sampling_percent=30)\n",
        "sampled_indices = sampler.sample_indices()\n",
        "X_train1_6 = X_train1.iloc[sampled_indices]\n",
        "y_train1_6 = y_train1.iloc[sampled_indices]\n",
        "print(\"Sampled Data:\", len(sampled_indices),\"Data:\",len(data))\n",
        "print(\"Time taken for Random Sampling:\", time.time() - start_time, \"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WrPoyf0h4Hrv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a7eb5c2-e7bd-417d-bb5c-7bbd18f7b0ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DropHighCorrelation: 0 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 0 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 0 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 0 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 0 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 0 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 0 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 0 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 0 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 0 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 0 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 0 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 0 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 0 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 0 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 0 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 0 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 0 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 0 columns will be dropped due to correlation (threshold=0.6).\n",
            "DropHighCorrelation: 0 columns will be dropped due to correlation (threshold=0.6).\n"
          ]
        }
      ],
      "source": [
        "# ================================ #\n",
        "##### Trainer for All State Sampling Methods\n",
        "# ================================ #\n",
        "trainer = ModelTrainer(\n",
        "    datasets_X=[\n",
        "        [X_train1_1, X_test1],\n",
        "        # [X_train1_2, X_test1],\n",
        "        [X_train1_3, X_test1],\n",
        "        [X_train1_4, X_test1],\n",
        "        [X_train1_5, X_test1],\n",
        "        [X_train1_6, X_test1]\n",
        "    ],\n",
        "    datasets_y=[\n",
        "        [y_train1_1, y_test1],\n",
        "        # [y_train1_2, y_test1],\n",
        "        [y_train1_3, y_test1],\n",
        "        [y_train1_4, y_test1],\n",
        "        [y_train1_5, y_test1],\n",
        "        [y_train1_6, y_test1]\n",
        "    ],\n",
        "    task_type='classification',\n",
        "    dataset_names=[\n",
        "        'Dim Red and Grid Search',\n",
        "        'Isolation Forest and KS Sampling',\n",
        "        'Entropy Sampling',\n",
        "        'Distance Based',\n",
        "        'Random'\n",
        "    ]\n",
        ")\n",
        "\n",
        "results = trainer.train_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hJNQQkUc60mx"
      },
      "outputs": [],
      "source": [
        "results.to_csv(\"Credit Card.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "QtjGJOybmDxi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e0c481a7-5862-45c1-fcf7-331a13d4938c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                  Train F1   Test F1  Train AUC  Test AUC  \\\n",
              "Dataset                                                                     \n",
              "Dim Red and Grid Search           0.532751  0.274143   1.000000  0.957368   \n",
              "Dim Red and Grid Search           0.053476  0.048999   0.923914  0.881650   \n",
              "Dim Red and Grid Search           0.239156  0.256776   0.966934  0.982485   \n",
              "Dim Red and Grid Search           0.117364  0.107020   1.000000  0.986547   \n",
              "Isolation Forest and KS Sampling  0.535714  0.280528   1.000000  0.936790   \n",
              "Isolation Forest and KS Sampling  0.037283  0.028846   0.999958  0.906743   \n",
              "Isolation Forest and KS Sampling  0.072595  0.065735   0.976289  0.957443   \n",
              "Isolation Forest and KS Sampling  0.171429  0.144040   1.000000  0.958448   \n",
              "Entropy Sampling                  0.162679  0.097832   1.000000  0.946742   \n",
              "Entropy Sampling                  0.025494  0.075065   0.983721  0.949563   \n",
              "Entropy Sampling                  0.033733  0.128023   0.969955  0.979164   \n",
              "Entropy Sampling                  0.047842  0.114035   1.000000  0.978961   \n",
              "Distance Based                    0.662379  0.294416   1.000000  0.952304   \n",
              "Distance Based                    0.325662  0.218750   0.959806  0.945362   \n",
              "Distance Based                    0.180113  0.118110   0.984314  0.978222   \n",
              "Distance Based                    0.425034  0.284360   1.000000  0.986904   \n",
              "Random                            0.198234  0.107232   1.000000  0.936717   \n",
              "Random                            0.779487  0.638743   0.851422  0.744785   \n",
              "Random                            0.207885  0.242507   0.957992  0.976338   \n",
              "Random                            0.114708  0.120498   1.000000  0.981165   \n",
              "\n",
              "                                  Train Accuracy  Test Accuracy  Train Recall  \\\n",
              "Dataset                                                                         \n",
              "Dim Red and Grid Search                 0.996869       0.991819      1.000000   \n",
              "Dim Red and Grid Search                 0.945621       0.944121      0.860656   \n",
              "Dim Red and Grid Search                 0.990505       0.990854      0.836066   \n",
              "Dim Red and Grid Search                 0.973154       0.972754      1.000000   \n",
              "Isolation Forest and KS Sampling        0.996577       0.992346      1.000000   \n",
              "Isolation Forest and KS Sampling        0.897999       0.897160      1.000000   \n",
              "Isolation Forest and KS Sampling        0.955144       0.956585      0.888889   \n",
              "Isolation Forest and KS Sampling        0.980908       0.981848      1.000000   \n",
              "Entropy Sampling                        0.992319       0.971507      1.000000   \n",
              "Entropy Sampling                        0.945196       0.962361      0.960784   \n",
              "Entropy Sampling                        0.962284       0.978477      0.882353   \n",
              "Entropy Sampling                        0.970301       0.975176      1.000000   \n",
              "Distance Based                          0.995392       0.992679      1.000000   \n",
              "Distance Based                          0.982854       0.989467      0.915858   \n",
              "Distance Based                          0.961640       0.976405      0.932039   \n",
              "Distance Based                          0.987769       0.992047      1.000000   \n",
              "Random                                  0.988047       0.974860      1.000000   \n",
              "Random                                  0.999371       0.998789      0.752475   \n",
              "Random                                  0.990300       0.990239      0.861386   \n",
              "Random                                  0.977192       0.976423      1.000000   \n",
              "\n",
              "                                  Test Recall  Train Precision  \\\n",
              "Dataset                                                          \n",
              "Dim Red and Grid Search              0.897959         0.363095   \n",
              "Dim Red and Grid Search              0.836735         0.027595   \n",
              "Dim Red and Grid Search              0.918367         0.139535   \n",
              "Dim Red and Grid Search              0.948980         0.062340   \n",
              "Isolation Forest and KS Sampling     0.867347         0.365854   \n",
              "Isolation Forest and KS Sampling     0.887755         0.018995   \n",
              "Isolation Forest and KS Sampling     0.887755         0.037843   \n",
              "Isolation Forest and KS Sampling     0.887755         0.093750   \n",
              "Entropy Sampling                     0.897959         0.088542   \n",
              "Entropy Sampling                     0.887755         0.012919   \n",
              "Entropy Sampling                     0.918367         0.017195   \n",
              "Entropy Sampling                     0.928571         0.024507   \n",
              "Distance Based                       0.887755         0.495192   \n",
              "Distance Based                       0.857143         0.198041   \n",
              "Distance Based                       0.918367         0.099688   \n",
              "Distance Based                       0.918367         0.269869   \n",
              "Random                               0.877551         0.110022   \n",
              "Random                               0.622449         0.808511   \n",
              "Random                               0.908163         0.118207   \n",
              "Random                               0.938776         0.060843   \n",
              "\n",
              "                                  Test Precision   Best Cutoff  \\\n",
              "Dataset                                                          \n",
              "Dim Red and Grid Search                 0.161765  2.000000e-02   \n",
              "Dim Red and Grid Search                 0.025239  4.091485e-04   \n",
              "Dim Red and Grid Search                 0.149254  4.916399e-03   \n",
              "Dim Red and Grid Search                 0.056707  1.627373e-04   \n",
              "Isolation Forest and KS Sampling        0.167323  2.000000e-02   \n",
              "Isolation Forest and KS Sampling        0.014661  1.078216e-07   \n",
              "Isolation Forest and KS Sampling        0.034131  1.775125e-03   \n",
              "Isolation Forest and KS Sampling        0.078378  4.439231e-04   \n",
              "Entropy Sampling                        0.051734  1.000000e-02   \n",
              "Entropy Sampling                        0.039189  1.316299e-04   \n",
              "Entropy Sampling                        0.068807  7.488785e-04   \n",
              "Entropy Sampling                        0.060748  1.045149e-04   \n",
              "Distance Based                          0.176471  2.000000e-02   \n",
              "Distance Based                          0.125373  7.322867e-04   \n",
              "Distance Based                          0.063114  4.064947e-03   \n",
              "Distance Based                          0.168224  4.690796e-04   \n",
              "Random                                  0.057105  1.000000e-02   \n",
              "Random                                  0.655914  3.259643e-01   \n",
              "Random                                  0.139937  3.242756e-03   \n",
              "Random                                  0.064381  1.577422e-04   \n",
              "\n",
              "                                  Training Time (seconds)               Model  \n",
              "Dataset                                                                        \n",
              "Dim Red and Grid Search                         52.835446        RandomForest  \n",
              "Dim Red and Grid Search                        117.216803    GradientBoosting  \n",
              "Dim Red and Grid Search                          0.320317  LogisticRegression  \n",
              "Dim Red and Grid Search                          0.673176             XGBoost  \n",
              "Isolation Forest and KS Sampling                 9.249989        RandomForest  \n",
              "Isolation Forest and KS Sampling                34.908320    GradientBoosting  \n",
              "Isolation Forest and KS Sampling                 0.123132  LogisticRegression  \n",
              "Isolation Forest and KS Sampling                 0.244770             XGBoost  \n",
              "Entropy Sampling                                47.926316        RandomForest  \n",
              "Entropy Sampling                               118.060167    GradientBoosting  \n",
              "Entropy Sampling                                 0.345187  LogisticRegression  \n",
              "Entropy Sampling                                 0.575342             XGBoost  \n",
              "Distance Based                                  57.625243        RandomForest  \n",
              "Distance Based                                 118.166399    GradientBoosting  \n",
              "Distance Based                                   0.342963  LogisticRegression  \n",
              "Distance Based                                   0.694755             XGBoost  \n",
              "Random                                          53.892328        RandomForest  \n",
              "Random                                         117.725484    GradientBoosting  \n",
              "Random                                           0.354243  LogisticRegression  \n",
              "Random                                           1.948408             XGBoost  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-112298e8-44bb-4faf-ab72-0047e06c5555\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train F1</th>\n",
              "      <th>Test F1</th>\n",
              "      <th>Train AUC</th>\n",
              "      <th>Test AUC</th>\n",
              "      <th>Train Accuracy</th>\n",
              "      <th>Test Accuracy</th>\n",
              "      <th>Train Recall</th>\n",
              "      <th>Test Recall</th>\n",
              "      <th>Train Precision</th>\n",
              "      <th>Test Precision</th>\n",
              "      <th>Best Cutoff</th>\n",
              "      <th>Training Time (seconds)</th>\n",
              "      <th>Model</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dataset</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Dim Red and Grid Search</th>\n",
              "      <td>0.532751</td>\n",
              "      <td>0.274143</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.957368</td>\n",
              "      <td>0.996869</td>\n",
              "      <td>0.991819</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.897959</td>\n",
              "      <td>0.363095</td>\n",
              "      <td>0.161765</td>\n",
              "      <td>2.000000e-02</td>\n",
              "      <td>52.835446</td>\n",
              "      <td>RandomForest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dim Red and Grid Search</th>\n",
              "      <td>0.053476</td>\n",
              "      <td>0.048999</td>\n",
              "      <td>0.923914</td>\n",
              "      <td>0.881650</td>\n",
              "      <td>0.945621</td>\n",
              "      <td>0.944121</td>\n",
              "      <td>0.860656</td>\n",
              "      <td>0.836735</td>\n",
              "      <td>0.027595</td>\n",
              "      <td>0.025239</td>\n",
              "      <td>4.091485e-04</td>\n",
              "      <td>117.216803</td>\n",
              "      <td>GradientBoosting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dim Red and Grid Search</th>\n",
              "      <td>0.239156</td>\n",
              "      <td>0.256776</td>\n",
              "      <td>0.966934</td>\n",
              "      <td>0.982485</td>\n",
              "      <td>0.990505</td>\n",
              "      <td>0.990854</td>\n",
              "      <td>0.836066</td>\n",
              "      <td>0.918367</td>\n",
              "      <td>0.139535</td>\n",
              "      <td>0.149254</td>\n",
              "      <td>4.916399e-03</td>\n",
              "      <td>0.320317</td>\n",
              "      <td>LogisticRegression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dim Red and Grid Search</th>\n",
              "      <td>0.117364</td>\n",
              "      <td>0.107020</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.986547</td>\n",
              "      <td>0.973154</td>\n",
              "      <td>0.972754</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.948980</td>\n",
              "      <td>0.062340</td>\n",
              "      <td>0.056707</td>\n",
              "      <td>1.627373e-04</td>\n",
              "      <td>0.673176</td>\n",
              "      <td>XGBoost</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Isolation Forest and KS Sampling</th>\n",
              "      <td>0.535714</td>\n",
              "      <td>0.280528</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.936790</td>\n",
              "      <td>0.996577</td>\n",
              "      <td>0.992346</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.867347</td>\n",
              "      <td>0.365854</td>\n",
              "      <td>0.167323</td>\n",
              "      <td>2.000000e-02</td>\n",
              "      <td>9.249989</td>\n",
              "      <td>RandomForest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Isolation Forest and KS Sampling</th>\n",
              "      <td>0.037283</td>\n",
              "      <td>0.028846</td>\n",
              "      <td>0.999958</td>\n",
              "      <td>0.906743</td>\n",
              "      <td>0.897999</td>\n",
              "      <td>0.897160</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.887755</td>\n",
              "      <td>0.018995</td>\n",
              "      <td>0.014661</td>\n",
              "      <td>1.078216e-07</td>\n",
              "      <td>34.908320</td>\n",
              "      <td>GradientBoosting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Isolation Forest and KS Sampling</th>\n",
              "      <td>0.072595</td>\n",
              "      <td>0.065735</td>\n",
              "      <td>0.976289</td>\n",
              "      <td>0.957443</td>\n",
              "      <td>0.955144</td>\n",
              "      <td>0.956585</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.887755</td>\n",
              "      <td>0.037843</td>\n",
              "      <td>0.034131</td>\n",
              "      <td>1.775125e-03</td>\n",
              "      <td>0.123132</td>\n",
              "      <td>LogisticRegression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Isolation Forest and KS Sampling</th>\n",
              "      <td>0.171429</td>\n",
              "      <td>0.144040</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.958448</td>\n",
              "      <td>0.980908</td>\n",
              "      <td>0.981848</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.887755</td>\n",
              "      <td>0.093750</td>\n",
              "      <td>0.078378</td>\n",
              "      <td>4.439231e-04</td>\n",
              "      <td>0.244770</td>\n",
              "      <td>XGBoost</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Entropy Sampling</th>\n",
              "      <td>0.162679</td>\n",
              "      <td>0.097832</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.946742</td>\n",
              "      <td>0.992319</td>\n",
              "      <td>0.971507</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.897959</td>\n",
              "      <td>0.088542</td>\n",
              "      <td>0.051734</td>\n",
              "      <td>1.000000e-02</td>\n",
              "      <td>47.926316</td>\n",
              "      <td>RandomForest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Entropy Sampling</th>\n",
              "      <td>0.025494</td>\n",
              "      <td>0.075065</td>\n",
              "      <td>0.983721</td>\n",
              "      <td>0.949563</td>\n",
              "      <td>0.945196</td>\n",
              "      <td>0.962361</td>\n",
              "      <td>0.960784</td>\n",
              "      <td>0.887755</td>\n",
              "      <td>0.012919</td>\n",
              "      <td>0.039189</td>\n",
              "      <td>1.316299e-04</td>\n",
              "      <td>118.060167</td>\n",
              "      <td>GradientBoosting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Entropy Sampling</th>\n",
              "      <td>0.033733</td>\n",
              "      <td>0.128023</td>\n",
              "      <td>0.969955</td>\n",
              "      <td>0.979164</td>\n",
              "      <td>0.962284</td>\n",
              "      <td>0.978477</td>\n",
              "      <td>0.882353</td>\n",
              "      <td>0.918367</td>\n",
              "      <td>0.017195</td>\n",
              "      <td>0.068807</td>\n",
              "      <td>7.488785e-04</td>\n",
              "      <td>0.345187</td>\n",
              "      <td>LogisticRegression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Entropy Sampling</th>\n",
              "      <td>0.047842</td>\n",
              "      <td>0.114035</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.978961</td>\n",
              "      <td>0.970301</td>\n",
              "      <td>0.975176</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.024507</td>\n",
              "      <td>0.060748</td>\n",
              "      <td>1.045149e-04</td>\n",
              "      <td>0.575342</td>\n",
              "      <td>XGBoost</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Distance Based</th>\n",
              "      <td>0.662379</td>\n",
              "      <td>0.294416</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.952304</td>\n",
              "      <td>0.995392</td>\n",
              "      <td>0.992679</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.887755</td>\n",
              "      <td>0.495192</td>\n",
              "      <td>0.176471</td>\n",
              "      <td>2.000000e-02</td>\n",
              "      <td>57.625243</td>\n",
              "      <td>RandomForest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Distance Based</th>\n",
              "      <td>0.325662</td>\n",
              "      <td>0.218750</td>\n",
              "      <td>0.959806</td>\n",
              "      <td>0.945362</td>\n",
              "      <td>0.982854</td>\n",
              "      <td>0.989467</td>\n",
              "      <td>0.915858</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.198041</td>\n",
              "      <td>0.125373</td>\n",
              "      <td>7.322867e-04</td>\n",
              "      <td>118.166399</td>\n",
              "      <td>GradientBoosting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Distance Based</th>\n",
              "      <td>0.180113</td>\n",
              "      <td>0.118110</td>\n",
              "      <td>0.984314</td>\n",
              "      <td>0.978222</td>\n",
              "      <td>0.961640</td>\n",
              "      <td>0.976405</td>\n",
              "      <td>0.932039</td>\n",
              "      <td>0.918367</td>\n",
              "      <td>0.099688</td>\n",
              "      <td>0.063114</td>\n",
              "      <td>4.064947e-03</td>\n",
              "      <td>0.342963</td>\n",
              "      <td>LogisticRegression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Distance Based</th>\n",
              "      <td>0.425034</td>\n",
              "      <td>0.284360</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.986904</td>\n",
              "      <td>0.987769</td>\n",
              "      <td>0.992047</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.918367</td>\n",
              "      <td>0.269869</td>\n",
              "      <td>0.168224</td>\n",
              "      <td>4.690796e-04</td>\n",
              "      <td>0.694755</td>\n",
              "      <td>XGBoost</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random</th>\n",
              "      <td>0.198234</td>\n",
              "      <td>0.107232</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.936717</td>\n",
              "      <td>0.988047</td>\n",
              "      <td>0.974860</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.877551</td>\n",
              "      <td>0.110022</td>\n",
              "      <td>0.057105</td>\n",
              "      <td>1.000000e-02</td>\n",
              "      <td>53.892328</td>\n",
              "      <td>RandomForest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random</th>\n",
              "      <td>0.779487</td>\n",
              "      <td>0.638743</td>\n",
              "      <td>0.851422</td>\n",
              "      <td>0.744785</td>\n",
              "      <td>0.999371</td>\n",
              "      <td>0.998789</td>\n",
              "      <td>0.752475</td>\n",
              "      <td>0.622449</td>\n",
              "      <td>0.808511</td>\n",
              "      <td>0.655914</td>\n",
              "      <td>3.259643e-01</td>\n",
              "      <td>117.725484</td>\n",
              "      <td>GradientBoosting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random</th>\n",
              "      <td>0.207885</td>\n",
              "      <td>0.242507</td>\n",
              "      <td>0.957992</td>\n",
              "      <td>0.976338</td>\n",
              "      <td>0.990300</td>\n",
              "      <td>0.990239</td>\n",
              "      <td>0.861386</td>\n",
              "      <td>0.908163</td>\n",
              "      <td>0.118207</td>\n",
              "      <td>0.139937</td>\n",
              "      <td>3.242756e-03</td>\n",
              "      <td>0.354243</td>\n",
              "      <td>LogisticRegression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random</th>\n",
              "      <td>0.114708</td>\n",
              "      <td>0.120498</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.981165</td>\n",
              "      <td>0.977192</td>\n",
              "      <td>0.976423</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.938776</td>\n",
              "      <td>0.060843</td>\n",
              "      <td>0.064381</td>\n",
              "      <td>1.577422e-04</td>\n",
              "      <td>1.948408</td>\n",
              "      <td>XGBoost</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-112298e8-44bb-4faf-ab72-0047e06c5555')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-112298e8-44bb-4faf-ab72-0047e06c5555 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-112298e8-44bb-4faf-ab72-0047e06c5555');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9dae8ef1-4eab-4b90-9af4-4b6922b16991\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9dae8ef1-4eab-4b90-9af4-4b6922b16991')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9dae8ef1-4eab-4b90-9af4-4b6922b16991 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_c8a00936-3b47-4006-8eff-d1d8ad429c79\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c8a00936-3b47-4006-8eff-d1d8ad429c79 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results",
              "summary": "{\n  \"name\": \"results\",\n  \"rows\": 20,\n  \"fields\": [\n    {\n      \"column\": \"Dataset\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Isolation Forest and KS Sampling\",\n          \"Random\",\n          \"Entropy Sampling\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Train F1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.22485017098719443,\n        \"min\": 0.025494276795005204,\n        \"max\": 0.7794871794871795,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.5327510917030568,\n          0.7794871794871795,\n          0.4250343878954608\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Test F1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.13798590773011496,\n        \"min\": 0.028846153846153848,\n        \"max\": 0.6387434554973822,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.27414330218068533,\n          0.6387434554973822,\n          0.2843601895734597\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Train AUC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.036456992298198154,\n        \"min\": 0.8514224803480172,\n        \"max\": 1.0,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          0.9837209429172976,\n          1.0,\n          0.8514224803480172\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Test AUC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0546943676563835,\n        \"min\": 0.7447854458328069,\n        \"max\": 0.9869039484111033,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.9573682965729904,\n          0.7447854458328069,\n          0.9869039484111033\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Train Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.024706376272568626,\n        \"min\": 0.897998595505618,\n        \"max\": 0.999370912761693,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.9968691937442394,\n          0.999370912761693,\n          0.9877693736924494\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Test Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.022982526452333684,\n        \"min\": 0.897159509848671,\n        \"max\": 0.9987886661282961,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.9918191074751589,\n          0.9987886661282961,\n          0.9920473297988133\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Train Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07421439098357525,\n        \"min\": 0.7524752475247525,\n        \"max\": 1.0,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.7524752475247525,\n          0.860655737704918,\n          0.8823529411764706\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Test Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06753221268479305,\n        \"min\": 0.6224489795918368,\n        \"max\": 0.9489795918367347,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          0.9081632653061225,\n          0.6224489795918368,\n          0.8979591836734694\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Train Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.20231405485793455,\n        \"min\": 0.012918534141840231,\n        \"max\": 0.8085106382978723,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.3630952380952381,\n          0.8085106382978723,\n          0.2698689956331878\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Test Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.13739937067529562,\n        \"min\": 0.014661274014155713,\n        \"max\": 0.6559139784946236,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.16176470588235295,\n          0.6559139784946236,\n          0.16822429906542055\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Best Cutoff\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0720923064867988,\n        \"min\": 1.0782157423620783e-07,\n        \"max\": 0.32596431244106877,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          0.02,\n          0.0004091485195417274,\n          0.0017751249196512355\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Training Time (seconds)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 46.59041613528135,\n        \"min\": 0.12313151359558105,\n        \"max\": 118.16639924049377,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          52.83544611930847,\n          117.72548365592957,\n          0.6947550773620605\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"GradientBoosting\",\n          \"XGBoost\",\n          \"RandomForest\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOxz86XOf3HjCV+KxcKg0Kf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}