{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vibhuverma17/MLBASEDSAMPLING/blob/main/All_Claims_Sampling_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aad7i3jF5Rcf",
        "outputId": "bb8e642d-4803-476d-ab45-77b59d4fbc80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.10/dist-packages (0.5.7)\n",
            "Requirement already satisfied: hdbscan in /usr/local/lib/python3.10/dist-packages (0.8.40)\n",
            "Requirement already satisfied: gower in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: kmodes in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: XGBoost in /usr/local/lib/python3.10/dist-packages (2.1.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.5.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.5.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.6)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan) (1.4.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from XGBoost) (2.23.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install umap-learn hdbscan gower kmodes XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ov6EwiM05WKy"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# Standard Libraries\n",
        "# ===============================\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import ast\n",
        "\n",
        "# ===============================\n",
        "# Visualization Libraries\n",
        "# ===============================\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "\n",
        "# ===============================\n",
        "# Scikit-learn Components\n",
        "# ===============================\n",
        "# Data Splitting and Preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Models\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, RandomForestRegressor,\n",
        "    GradientBoostingClassifier, GradientBoostingRegressor,\n",
        "    IsolationForest\n",
        ")\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import (\n",
        "    f1_score, roc_auc_score, accuracy_score, recall_score, precision_score,\n",
        "    mean_absolute_error, mean_squared_error, r2_score,\n",
        "    mean_absolute_percentage_error, roc_curve\n",
        ")\n",
        "\n",
        "# Pairwise distances\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "\n",
        "# ===============================\n",
        "# External Libraries\n",
        "# ===============================\n",
        "import xgboost as xgb  # XGBoost library\n",
        "from scipy.stats import ks_2samp, entropy  # Statistical tests\n",
        "from scipy.spatial.distance import cdist\n",
        "from kmodes.kprototypes import KPrototypes  # Clustering\n",
        "import umap  # Dimensionality reduction\n",
        "import hdbscan  # Density-based clustering\n",
        "import gower  # Gower similarity for mixed data types\n",
        "\n",
        "# ===============================\n",
        "# Configure Warnings\n",
        "# ===============================\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBV8SEqcwY-L"
      },
      "source": [
        "#### DIM REDUCTION AND GRID SEARCH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQsa-dNvH2vc"
      },
      "outputs": [],
      "source": [
        "class StratifiedSamplingUMAP:\n",
        "    def __init__(self, n_neighbors=15, min_dist=0.1, n_components=2, n_grids=10, sample_percentage=0.1):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.min_dist = min_dist\n",
        "        self.n_components = n_components\n",
        "        self.n_grids = n_grids\n",
        "        self.sample_percentage = sample_percentage\n",
        "        self.sample_indices = None  # To store sampled indices\n",
        "\n",
        "    def check_categorical(self, data):\n",
        "        return data.select_dtypes(include=['object']).shape[1] > 0\n",
        "\n",
        "    def fit_transform(self, data):\n",
        "        if self.check_categorical(data):\n",
        "            print(\"Categorical variables detected. Using 'dice' metric for UMAP.\")\n",
        "            categorical_data = data.select_dtypes(include=['object'])\n",
        "            encoder = OneHotEncoder(sparse_output=False)\n",
        "            categorical_encoded = encoder.fit_transform(categorical_data)\n",
        "            continuous_data = data.select_dtypes(exclude=['object'])\n",
        "            combined_data = np.hstack([categorical_encoded, continuous_data])\n",
        "            metric = 'dice'\n",
        "        else:\n",
        "            print(\"No categorical variables detected. Using 'euclidean' metric for UMAP.\")\n",
        "            combined_data = data\n",
        "            metric = 'euclidean'\n",
        "\n",
        "        umap_model = umap.UMAP(n_neighbors=self.n_neighbors, min_dist=self.min_dist,\n",
        "                               n_components=self.n_components, metric=metric)\n",
        "        return umap_model.fit_transform(combined_data)\n",
        "\n",
        "    def stratified_sampling(self, embedding):\n",
        "        min_x, min_y = np.min(embedding[:, :2], axis=0)\n",
        "        max_x, max_y = np.max(embedding[:, :2], axis=0)\n",
        "\n",
        "        x_bins = np.linspace(min_x, max_x, self.n_grids)\n",
        "        y_bins = np.linspace(min_y, max_y, self.n_grids)\n",
        "\n",
        "        if self.n_components == 3:\n",
        "            min_z = np.min(embedding[:, 2])\n",
        "            max_z = np.max(embedding[:, 2])\n",
        "            z_bins = np.linspace(min_z, max_z, self.n_grids)\n",
        "            grid_counts = np.zeros((self.n_grids, self.n_grids, self.n_grids))\n",
        "        else:\n",
        "            grid_counts = np.zeros((self.n_grids, self.n_grids))\n",
        "\n",
        "        for point in embedding:\n",
        "            x_idx = np.digitize(point[0], x_bins) - 1\n",
        "            y_idx = np.digitize(point[1], y_bins) - 1\n",
        "\n",
        "            if self.n_components == 3:\n",
        "                z_idx = np.digitize(point[2], z_bins) - 1\n",
        "                grid_counts[x_idx, y_idx, z_idx] += 1\n",
        "            else:\n",
        "                grid_counts[x_idx, y_idx] += 1\n",
        "\n",
        "        grid_probs = grid_counts / np.sum(grid_counts)\n",
        "        grid_probs_flat = grid_probs.flatten()\n",
        "\n",
        "        # Sample indices based on the probability distribution\n",
        "        sampled_indices = np.random.choice(len(embedding), size=int(len(embedding) * self.sample_percentage), replace=False)\n",
        "\n",
        "        # Store the sampled indices for later retrieval\n",
        "        self.sample_indices = sampled_indices\n",
        "\n",
        "        # Extract the corresponding samples from the embedding\n",
        "        sampled_embedding = embedding[sampled_indices]\n",
        "\n",
        "        return sampled_embedding\n",
        "\n",
        "    def get_sample_indices(self):\n",
        "        \"\"\"\n",
        "        Return the indices of the sampled data points in the original dataset.\n",
        "        \"\"\"\n",
        "        if self.sample_indices is None:\n",
        "            raise ValueError(\"No samples have been selected. Please run stratified_sampling first.\")\n",
        "        return self.sample_indices\n",
        "\n",
        "    def plot(self, embedding):\n",
        "        if self.n_components == 2:\n",
        "            plt.scatter(embedding[:, 0], embedding[:, 1], c='blue', marker='o')\n",
        "            plt.title('UMAP Projection (2D)')\n",
        "            plt.xlabel('UMAP 1')\n",
        "            plt.ylabel('UMAP 2')\n",
        "            plt.show()\n",
        "\n",
        "        elif self.n_components == 3:\n",
        "            fig = plt.figure()\n",
        "            ax = fig.add_subplot(111, projection='3d')\n",
        "            ax.scatter(embedding[:, 0], embedding[:, 1], embedding[:, 2], c='blue', marker='o')\n",
        "            ax.set_title('UMAP Projection (3D)')\n",
        "            ax.set_xlabel('UMAP 1')\n",
        "            ax.set_ylabel('UMAP 2')\n",
        "            ax.set_zlabel('UMAP 3')\n",
        "            plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eao-WLtwwS_7"
      },
      "source": [
        "##### HDBSCAN - CLUSTERING BASED SAMPLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emjvmMeDMmhj"
      },
      "outputs": [],
      "source": [
        "class ClusterSampler:\n",
        "    def __init__(self, data, sampling_percent=10, **hdbscan_params):\n",
        "        \"\"\"\n",
        "        Initialize the ClusterSampler class.\n",
        "\n",
        "        Parameters:\n",
        "        - data: DataFrame containing the dataset\n",
        "        - sampling_percent: Percentage of points to sample from each cluster (0-100)\n",
        "        - **hdbscan_params: Additional parameters to pass to HDBSCAN\n",
        "        \"\"\"\n",
        "        self.data = self._convert_data_types(data)\n",
        "        self.sampling_percent = sampling_percent\n",
        "        self.is_categorical = self._detect_categorical(data)\n",
        "        self.cluster_labels = None\n",
        "        self.hdbscan_params = hdbscan_params\n",
        "\n",
        "    def _convert_data_types(self, data):\n",
        "        \"\"\"Ensure continuous columns are float64 and categorical columns are object.\"\"\"\n",
        "        continuous_cols = data.select_dtypes(include=['float', 'int']).columns\n",
        "        data[continuous_cols] = data[continuous_cols].astype(np.float64)\n",
        "\n",
        "        categorical_cols = data.select_dtypes(include=['object', 'category']).columns\n",
        "        data[categorical_cols] = data[categorical_cols].astype('object')\n",
        "\n",
        "        return data\n",
        "\n",
        "    def _detect_categorical(self, data):\n",
        "        \"\"\"Detect if the dataset contains categorical features.\"\"\"\n",
        "        return data.select_dtypes(include=['object', 'category']).shape[1] > 0\n",
        "\n",
        "    def _compute_distance_matrix(self):\n",
        "        \"\"\"Compute the distance matrix based on the data type.\"\"\"\n",
        "        if self.is_categorical:\n",
        "            gower_matrix = gower.gower_matrix(self.data)\n",
        "            return gower_matrix.astype(np.float64)\n",
        "        else:\n",
        "            return pairwise_distances(self.data, metric='euclidean')\n",
        "\n",
        "    def fit_clusters(self):\n",
        "        \"\"\"Fit HDBSCAN on the dataset with appropriate distance metric.\"\"\"\n",
        "        distance_matrix = self._compute_distance_matrix()\n",
        "        clusterer = hdbscan.HDBSCAN(metric='precomputed' if self.is_categorical else 'euclidean', **self.hdbscan_params)\n",
        "        self.cluster_labels = clusterer.fit_predict(distance_matrix)\n",
        "\n",
        "    def sample_points(self):\n",
        "        \"\"\"Sample a representative subset of points from each cluster, including noise points as a separate cluster.\"\"\"\n",
        "        if self.cluster_labels is None:\n",
        "            raise ValueError(\"Clusters have not been computed. Call fit_clusters() first.\")\n",
        "\n",
        "        data_with_labels = self.data.copy()\n",
        "        data_with_labels['cluster'] = self.cluster_labels\n",
        "\n",
        "        sampled_indices = []\n",
        "        unique_labels = np.unique(self.cluster_labels)\n",
        "\n",
        "        for cluster_label in unique_labels:\n",
        "            cluster_indices = data_with_labels[data_with_labels['cluster'] == cluster_label].index\n",
        "            sample_size = max(1, int(len(cluster_indices) * (self.sampling_percent / 100)))\n",
        "\n",
        "            # Avoid sampling more points than available\n",
        "            if len(cluster_indices) < sample_size:\n",
        "                print(f\"Cluster {cluster_label} has only {len(cluster_indices)} points, sampling {len(cluster_indices)}.\")\n",
        "            sampled_indices.extend(np.random.choice(cluster_indices, min(sample_size, len(cluster_indices)), replace=False))\n",
        "\n",
        "        # Ensure sampled indices are unique and valid\n",
        "        sampled_indices = list(set(sampled_indices))\n",
        "        sampled_indices = [idx for idx in sampled_indices if idx < len(self.data)]\n",
        "\n",
        "        return sampled_indices\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Execute the full sampling pipeline: cluster, then sample.\"\"\"\n",
        "        self.fit_clusters()\n",
        "        return self.sample_points()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGWSKfBWxJNc"
      },
      "source": [
        "##### ISOLATION FOREST AND KS STATISTIC SAMPLER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4BSBb9W-9kc"
      },
      "outputs": [],
      "source": [
        "class AnomalySampler:\n",
        "    def __init__(self, X, sample_weight=None):\n",
        "        \"\"\"\n",
        "        Initialize the AnomalySampler class.\n",
        "\n",
        "        Parameters:\n",
        "        - X: DataFrame containing the dataset\n",
        "        - sample_weight: Optional sample weights for the Isolation Forest\n",
        "        \"\"\"\n",
        "        self.original_data = X\n",
        "        self.X = self._one_hot_encode(X)\n",
        "        self.sample_weight = sample_weight\n",
        "        self.preds = self.isolation_forest(self.X, sample_weight)\n",
        "\n",
        "    def _one_hot_encode(self, data):\n",
        "        \"\"\"\n",
        "        Apply one-hot encoding to categorical columns.\n",
        "\n",
        "        Parameters:\n",
        "        - data: DataFrame containing the dataset\n",
        "\n",
        "        Returns:\n",
        "        - One-hot encoded DataFrame\n",
        "        \"\"\"\n",
        "        return pd.get_dummies(data, drop_first=True)  # drop_first to avoid multicollinearity, if relevant\n",
        "\n",
        "    @staticmethod\n",
        "    def isolation_forest(X, sample_weight=None):\n",
        "        \"\"\"\n",
        "        Fits an Isolation Forest to the dataset and assigns an anomaly score to each sample.\n",
        "\n",
        "        Parameters:\n",
        "        - X: DataFrame or array-like containing the dataset\n",
        "        - sample_weight: Optional sample weights for the Isolation Forest\n",
        "\n",
        "        Returns:\n",
        "        - preds: Anomaly scores for each sample\n",
        "        \"\"\"\n",
        "        clf = IsolationForest().fit(X, sample_weight=sample_weight)\n",
        "        preds = clf.score_samples(X)\n",
        "        return preds\n",
        "\n",
        "    @staticmethod\n",
        "    def get_5_percent(num):\n",
        "        \"\"\"Calculate 5% of a given number.\"\"\"\n",
        "        return round(5 / 100 * num)\n",
        "\n",
        "    def get_5_percent_splits(self, length):\n",
        "        \"\"\"Splits a given length into 5% intervals.\"\"\"\n",
        "        five_percent = self.get_5_percent(length)\n",
        "        return np.arange(five_percent, length, five_percent)\n",
        "\n",
        "    def find_sample_indices(self):\n",
        "        \"\"\"\n",
        "        Finds a sample by comparing the distribution of anomaly scores between the sample\n",
        "        and the original distribution using the KS-test. Starts with a 5% sample, increasing\n",
        "        by 5% increments until a significant sample (p-value > 0.95) is found or a limit is reached.\n",
        "\n",
        "        Returns:\n",
        "        - List of indices representing the sample in the original dataset\n",
        "        \"\"\"\n",
        "        size_splits = self.get_5_percent_splits(len(self.X))\n",
        "        element = 1\n",
        "        iteration = 0\n",
        "\n",
        "        while element < len(size_splits):\n",
        "            sample_size = size_splits[element]\n",
        "            sample_indices = np.random.choice(np.arange(self.preds.size), size=sample_size, replace=False)\n",
        "            sample = np.take(self.preds, sample_indices)\n",
        "\n",
        "            # Check if KS test p-value indicates similar distributions\n",
        "            if ks_2samp(self.preds, sample).pvalue > 0.95:\n",
        "                return sample_indices  # Return indices from the original dataset\n",
        "\n",
        "            iteration += 1\n",
        "            if iteration >= 20:\n",
        "                element += 1\n",
        "                iteration = 0\n",
        "\n",
        "        # If no suitable sample is found, return the last attempted sample indices\n",
        "        return sample_indices\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRaDp-XhxRho"
      },
      "source": [
        "#### ENTROPY SAMPLER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a80f3-ir6gR"
      },
      "outputs": [],
      "source": [
        "class EntropySampler:\n",
        "    def __init__(self, data, sampling_percent=10, bins=10):\n",
        "        \"\"\"\n",
        "        Initialize the EntropySampler class.\n",
        "\n",
        "        Parameters:\n",
        "        - data: DataFrame containing the dataset\n",
        "        - sampling_percent: Percentage of points to sample based on entropy (0-100)\n",
        "        - bins: Number of bins to use for continuous data entropy calculation\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.sampling_percent = sampling_percent\n",
        "        self.bins = bins\n",
        "        self.entropy_scores = None\n",
        "\n",
        "    def _calculate_entropy(self):\n",
        "        \"\"\"\n",
        "        Calculate the entropy for each feature and aggregate entropy per data point.\n",
        "\n",
        "        Returns:\n",
        "        - entropy_scores: Array of entropy scores for each data point\n",
        "        \"\"\"\n",
        "        entropy_scores = np.zeros(len(self.data))\n",
        "\n",
        "        for col in self.data.columns:\n",
        "            if pd.api.types.is_numeric_dtype(self.data[col]):\n",
        "                # For continuous data, bin it and calculate entropy over the bins\n",
        "                counts, _ = np.histogram(self.data[col], bins=self.bins)\n",
        "                feature_entropy = entropy(counts + 1e-10)  # Add small value to avoid log(0)\n",
        "                feature_contributions = np.digitize(self.data[col], bins=np.histogram_bin_edges(self.data[col], bins=self.bins))\n",
        "            else:\n",
        "                # For categorical data, calculate entropy over unique values\n",
        "                counts = self.data[col].value_counts().values\n",
        "                feature_entropy = entropy(counts + 1e-10)\n",
        "                feature_contributions = self.data[col].map(self.data[col].value_counts(normalize=True)).values\n",
        "\n",
        "            # Accumulate entropy scores based on the feature contributions for each data point\n",
        "            entropy_scores += feature_contributions * feature_entropy\n",
        "\n",
        "        self.entropy_scores = entropy_scores\n",
        "\n",
        "    def sample_indices(self):\n",
        "        \"\"\"\n",
        "        Get indices of points with the highest entropy scores.\n",
        "\n",
        "        Returns:\n",
        "        - List of indices for sampled points based on entropy scores\n",
        "        \"\"\"\n",
        "        if self.entropy_scores is None:\n",
        "            self._calculate_entropy()\n",
        "\n",
        "        # Determine the number of points to sample\n",
        "        num_samples = max(1, int(len(self.data) * (self.sampling_percent / 100)))\n",
        "\n",
        "        # Get the indices of the top entropy scores\n",
        "        top_indices = np.argsort(self.entropy_scores)[-num_samples:]\n",
        "\n",
        "        return top_indices\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Execute the full entropy-based sampling process.\n",
        "\n",
        "        Returns:\n",
        "        - List of indices for sampled points based on entropy scores\n",
        "        \"\"\"\n",
        "        self._calculate_entropy()\n",
        "        return self.sample_indices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8XnIZ2O33UO"
      },
      "source": [
        "### DISTANCE BASED SAMPLER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qgqGwih34r9"
      },
      "outputs": [],
      "source": [
        "class DistanceBasedSampler:\n",
        "    def __init__(self, data, k=5, sampling_percent=10):\n",
        "        self.data = data.reset_index(drop=True)  # Reset index for consistent access\n",
        "        self.k = k\n",
        "        self.sampling_percent = sampling_percent\n",
        "        self.cluster_labels = None\n",
        "        self.cluster_centroids = None\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        \"\"\"Ensure all categorical columns are treated as strings.\"\"\"\n",
        "        self.data = self.data.apply(lambda col: col.astype(str) if col.dtype == 'object' else col)\n",
        "        return self.data\n",
        "\n",
        "    def fit_clusters(self):\n",
        "        \"\"\"\n",
        "        Fit clusters using K-Prototypes if categorical data is present,\n",
        "        otherwise fallback to K-Means for purely numerical data.\n",
        "        \"\"\"\n",
        "        self._prepare_data()\n",
        "        data_array = self.data.values\n",
        "\n",
        "        # Identify categorical column indices\n",
        "        categorical_indices = [\n",
        "            i for i, col in enumerate(self.data.columns) if self.data.dtypes[col] == 'object'\n",
        "        ]\n",
        "\n",
        "        if len(categorical_indices) == 0:\n",
        "            # No categorical data, use K-Means\n",
        "            print(\"No categorical data detected. Using K-Means instead.\")\n",
        "            kmeans = KMeans(n_clusters=self.k, random_state=42)\n",
        "            clusters = kmeans.fit_predict(data_array)\n",
        "            self.cluster_labels = clusters\n",
        "            self.cluster_centroids = kmeans.cluster_centers_\n",
        "        else:\n",
        "            # Use K-Prototypes for mixed data\n",
        "            kproto = KPrototypes(n_clusters=self.k, init='Cao', random_state=42)\n",
        "            clusters = kproto.fit_predict(data_array, categorical=categorical_indices)\n",
        "            self.cluster_labels = clusters\n",
        "            self.cluster_centroids = kproto.cluster_centroids_\n",
        "\n",
        "        return clusters\n",
        "\n",
        "    def _calculate_mixed_distance(self, point, centroids):\n",
        "        \"\"\"\n",
        "        Calculate the mixed distance (numerical + categorical) from a point to centroids.\n",
        "        \"\"\"\n",
        "        numeric_cols = self.data.select_dtypes(include=[np.number]).columns\n",
        "        categorical_cols = self.data.select_dtypes(include=['object']).columns\n",
        "\n",
        "        point_numeric = point[numeric_cols].values.astype(float)\n",
        "        point_categorical = point[categorical_cols].values\n",
        "\n",
        "        distances = []\n",
        "        for centroid in centroids:\n",
        "            # Split centroids into numeric and categorical parts\n",
        "            centroid_numeric = np.array(centroid[:len(numeric_cols)], dtype=float)\n",
        "            centroid_categorical = np.array(centroid[len(numeric_cols):])\n",
        "\n",
        "            # Calculate numerical distance (Euclidean)\n",
        "            numeric_distance = np.linalg.norm(point_numeric - centroid_numeric)\n",
        "            # Calculate categorical distance (Hamming)\n",
        "            categorical_distance = np.sum(point_categorical != centroid_categorical)\n",
        "\n",
        "            distances.append(numeric_distance + categorical_distance)\n",
        "\n",
        "        return np.array(distances)\n",
        "\n",
        "    def sample_indices(self):\n",
        "        \"\"\"\n",
        "        Identify sample indices based on distance to centroids.\n",
        "        \"\"\"\n",
        "        if self.cluster_labels is None:\n",
        "            self.fit_clusters()\n",
        "\n",
        "        centroids = self.cluster_centroids\n",
        "        distances = np.zeros(len(self.data))\n",
        "\n",
        "        for i, (_, point) in enumerate(self.data.iterrows()):\n",
        "            distances[i] = np.min(self._calculate_mixed_distance(point, centroids))\n",
        "\n",
        "        num_samples = max(1, int(len(self.data) * (self.sampling_percent / 100)))\n",
        "        sampled_indices = np.argsort(distances)[-num_samples:]\n",
        "\n",
        "        return sampled_indices\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Run the sampling process.\n",
        "        \"\"\"\n",
        "        self.fit_clusters()\n",
        "        return self.sample_indices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5w6E43q4zYH"
      },
      "source": [
        "#### RANDOM SAMPLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZclmsfb4vgc"
      },
      "outputs": [],
      "source": [
        "class RandomSampler:\n",
        "    def __init__(self, data, sampling_percent=10):\n",
        "        \"\"\"\n",
        "        Initialize the RandomSampler class.\n",
        "\n",
        "        Parameters:\n",
        "        - data: DataFrame containing the dataset\n",
        "        - sampling_percent: Percentage of rows to sample (0-100)\n",
        "        \"\"\"\n",
        "        self.data = data.reset_index(drop=True)  # Reset index to ensure consistency\n",
        "        self.sampling_percent = sampling_percent\n",
        "\n",
        "    def sample_indices(self):\n",
        "        \"\"\"\n",
        "        Randomly sample indices of rows from the dataset.\n",
        "\n",
        "        Returns:\n",
        "        - A list of sampled indices.\n",
        "        \"\"\"\n",
        "        if not (0 <= self.sampling_percent <= 100):\n",
        "            raise ValueError(\"sampling_percent must be between 0 and 100.\")\n",
        "\n",
        "        # Calculate the number of samples to draw\n",
        "        sample_size = int(len(self.data) * (self.sampling_percent / 100))\n",
        "        sample_size = max(1, sample_size)  # Ensure at least one index is selected\n",
        "\n",
        "        # Perform random sampling without replacement and return indices\n",
        "        sampled_indices = self.data.sample(n=sample_size, random_state=42).index.tolist()\n",
        "        return sampled_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C74qxcZZ42Rz"
      },
      "source": [
        "### MACHINE LEARNING MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9M0kBWKKsi6N"
      },
      "outputs": [],
      "source": [
        "# !mkdir Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuZdfS3kN2OZ"
      },
      "source": [
        "#### READING DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNwWlcPn-eqD"
      },
      "outputs": [],
      "source": [
        "# Custom transformer to drop highly correlated features\n",
        "class DropHighCorrelation(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, threshold=0.6):\n",
        "        \"\"\"\n",
        "        Custom transformer to drop highly correlated features.\n",
        "\n",
        "        Parameters:\n",
        "        - threshold: Correlation threshold above which features will be dropped.\n",
        "        \"\"\"\n",
        "        self.threshold = threshold\n",
        "        self.to_drop = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Identify features to drop based on the correlation threshold.\n",
        "        \"\"\"\n",
        "        corr_matrix = pd.DataFrame(X).corr().abs()  # Compute the absolute correlation matrix\n",
        "        upper_tri = corr_matrix.where(\n",
        "            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
        "        )  # Extract upper triangle\n",
        "        self.to_drop = [\n",
        "            column for column in upper_tri.columns if any(upper_tri[column] > self.threshold)\n",
        "        ]\n",
        "\n",
        "        print(f\"DropHighCorrelation: {len(self.to_drop)} columns will be dropped due to correlation (threshold={self.threshold}).\")\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Drop the identified features from the dataset.\n",
        "        \"\"\"\n",
        "        return pd.DataFrame(X).drop(columns=self.to_drop, errors='ignore')\n",
        "\n",
        "# Main ModelTrainer class\n",
        "class ModelTrainer:\n",
        "    def __init__(self, datasets_X, datasets_y, task_type, dataset_names=None):\n",
        "        \"\"\"\n",
        "        Initializes the ModelTrainer with datasets, explicitly provided task type, and dataset names.\n",
        "\n",
        "        Parameters:\n",
        "        - datasets_X: List of [X_train, X_test] for different datasets\n",
        "        - datasets_y: List of [y_train, y_test] for different datasets\n",
        "        - task_type: A string explicitly specifying the task type, either 'regression' or 'classification'.\n",
        "        - dataset_names: List of names for the datasets to be used as index in the results DataFrame\n",
        "        \"\"\"\n",
        "        if task_type not in ['classification', 'regression']:\n",
        "            raise ValueError(\"task_type must be either 'classification' or 'regression'\")\n",
        "\n",
        "        self.datasets_X = datasets_X\n",
        "        self.datasets_y = datasets_y\n",
        "        self.task_type = task_type\n",
        "        self.dataset_names = dataset_names\n",
        "\n",
        "    def _select_model(self):\n",
        "        \"\"\"Select model based on task type.\"\"\"\n",
        "        if self.task_type == 'classification':\n",
        "            return {\n",
        "                'RandomForest': RandomForestClassifier(random_state=42),\n",
        "                'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
        "                'LogisticRegression': LogisticRegression(),\n",
        "                'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "            }\n",
        "        elif self.task_type == 'regression':\n",
        "            return {\n",
        "                'RandomForest': RandomForestRegressor(random_state=42),\n",
        "                'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
        "                'LinearRegression': LinearRegression(),\n",
        "                'XGBoost': xgb.XGBRegressor(random_state=42)\n",
        "            }\n",
        "\n",
        "    def _create_pipeline(self, model, X):\n",
        "        \"\"\"Create a preprocessing and modeling pipeline.\"\"\"\n",
        "        # Identify categorical and numerical features\n",
        "        categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "        numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "        # Preprocessing for numerical features: Standard scaling\n",
        "        numerical_transformer = StandardScaler()\n",
        "\n",
        "        # Preprocessing for categorical features: One-hot encoding\n",
        "        categorical_transformer = OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False)\n",
        "\n",
        "        # Combine preprocessors in a column transformer\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numerical_transformer, numerical_features),\n",
        "                ('cat', categorical_transformer, categorical_features)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Define a pipeline with preprocessing, correlation dropping, and the specified model\n",
        "        pipeline = Pipeline(steps=[\n",
        "            ('preprocessor', preprocessor),  # First preprocess\n",
        "            ('drop_high_corr', DropHighCorrelation(threshold=0.6)),  # Then drop highly correlated features\n",
        "            ('model', model)  # Finally, apply the model\n",
        "        ])\n",
        "        return pipeline\n",
        "\n",
        "    def _get_best_cutoff(self, y_true, y_pred_proba):\n",
        "        \"\"\"Use Youden's J statistic to determine the best cutoff point for classification.\"\"\"\n",
        "        fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
        "        youden_index = tpr - fpr\n",
        "        best_cutoff = thresholds[np.argmax(youden_index)]\n",
        "        return best_cutoff\n",
        "\n",
        "    def _train_and_evaluate(self, model, X_train, X_test, y_train, y_test):\n",
        "        \"\"\"Train the model and evaluate it on both the training and test datasets.\"\"\"\n",
        "        start_time = time.time()\n",
        "        model.fit(X_train, y_train)\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        y_pred_train = model.predict(X_train)\n",
        "        y_pred_test = model.predict(X_test)\n",
        "\n",
        "        if self.task_type == 'classification':\n",
        "            y_pred_proba_test = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else y_pred_test\n",
        "            y_pred_proba_train = model.predict_proba(X_train)[:, 1] if hasattr(model, 'predict_proba') else y_pred_train\n",
        "\n",
        "            best_cutoff = self._get_best_cutoff(y_test, y_pred_proba_test)\n",
        "            y_pred_class_test = (y_pred_proba_test >= best_cutoff).astype(int)\n",
        "            y_pred_class_train = (y_pred_proba_train >= best_cutoff).astype(int)\n",
        "\n",
        "            metrics = {\n",
        "                'Train F1': f1_score(y_train, y_pred_class_train),\n",
        "                'Test F1': f1_score(y_test, y_pred_class_test),\n",
        "                'Train AUC': roc_auc_score(y_train, y_pred_proba_train),\n",
        "                'Test AUC': roc_auc_score(y_test, y_pred_proba_test),\n",
        "                'Train Accuracy': accuracy_score(y_train, y_pred_class_train),\n",
        "                'Test Accuracy': accuracy_score(y_test, y_pred_class_test),\n",
        "                'Train Recall': recall_score(y_train, y_pred_class_train),\n",
        "                'Test Recall': recall_score(y_test, y_pred_class_test),\n",
        "                'Train Precision': precision_score(y_train, y_pred_class_train),\n",
        "                'Test Precision': precision_score(y_test, y_pred_class_test),\n",
        "                'Best Cutoff': best_cutoff,\n",
        "                'Training Time (seconds)': training_time\n",
        "            }\n",
        "        else:\n",
        "            metrics = {\n",
        "                'Train MSE': mean_squared_error(y_train, y_pred_train),\n",
        "                'Test MSE': mean_squared_error(y_test, y_pred_test),\n",
        "                'Train MAPE': mean_absolute_percentage_error(y_train, y_pred_train),\n",
        "                'Test MAPE': mean_absolute_percentage_error(y_test, y_pred_test),\n",
        "                'Train R2': r2_score(y_train, y_pred_train),\n",
        "                'Test R2': r2_score(y_test, y_pred_test),\n",
        "                'Training Time (seconds)': training_time\n",
        "            }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def train_models(self):\n",
        "        \"\"\"Train and evaluate models on multiple datasets and return a DataFrame of results.\"\"\"\n",
        "        models = self._select_model()\n",
        "        results = []\n",
        "\n",
        "        for idx, (X_data, y_data) in enumerate(zip(self.datasets_X, self.datasets_y)):\n",
        "            X_train, X_test = X_data\n",
        "            y_train, y_test = y_data\n",
        "\n",
        "            for model_name, model in models.items():\n",
        "                pipeline = self._create_pipeline(model, X_train)\n",
        "                metrics = self._train_and_evaluate(pipeline, X_train, X_test, y_train, y_test)\n",
        "                metrics['Dataset'] = self.dataset_names[idx] if self.dataset_names else f'Dataset {idx+1}'\n",
        "                metrics['Model'] = model_name\n",
        "                results.append(metrics)\n",
        "\n",
        "        return pd.DataFrame(results).set_index('Dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "5m4zGB5rsjZU",
        "outputId": "be660ef5-8aa6-4ad1-82b7-c5ca54e801b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Data:\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-a05c5ca5-3f4e-4371-be48-bff5f72088ee\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cat1</th>\n",
              "      <th>cat2</th>\n",
              "      <th>cat3</th>\n",
              "      <th>cat4</th>\n",
              "      <th>cat5</th>\n",
              "      <th>cat6</th>\n",
              "      <th>cat7</th>\n",
              "      <th>cat8</th>\n",
              "      <th>cat9</th>\n",
              "      <th>cat10</th>\n",
              "      <th>...</th>\n",
              "      <th>cont5</th>\n",
              "      <th>cont6</th>\n",
              "      <th>cont7</th>\n",
              "      <th>cont8</th>\n",
              "      <th>cont9</th>\n",
              "      <th>cont10</th>\n",
              "      <th>cont11</th>\n",
              "      <th>cont12</th>\n",
              "      <th>cont13</th>\n",
              "      <th>cont14</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>...</td>\n",
              "      <td>0.310061</td>\n",
              "      <td>0.718367</td>\n",
              "      <td>0.335060</td>\n",
              "      <td>0.30260</td>\n",
              "      <td>0.67135</td>\n",
              "      <td>0.83510</td>\n",
              "      <td>0.569745</td>\n",
              "      <td>0.594646</td>\n",
              "      <td>0.822493</td>\n",
              "      <td>0.714843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "      <td>...</td>\n",
              "      <td>0.885834</td>\n",
              "      <td>0.438917</td>\n",
              "      <td>0.436585</td>\n",
              "      <td>0.60087</td>\n",
              "      <td>0.35127</td>\n",
              "      <td>0.43919</td>\n",
              "      <td>0.338312</td>\n",
              "      <td>0.366307</td>\n",
              "      <td>0.611431</td>\n",
              "      <td>0.304496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "      <td>...</td>\n",
              "      <td>0.397069</td>\n",
              "      <td>0.289648</td>\n",
              "      <td>0.315545</td>\n",
              "      <td>0.27320</td>\n",
              "      <td>0.26076</td>\n",
              "      <td>0.32446</td>\n",
              "      <td>0.381398</td>\n",
              "      <td>0.373424</td>\n",
              "      <td>0.195709</td>\n",
              "      <td>0.774425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>...</td>\n",
              "      <td>0.422268</td>\n",
              "      <td>0.440945</td>\n",
              "      <td>0.391128</td>\n",
              "      <td>0.31796</td>\n",
              "      <td>0.32128</td>\n",
              "      <td>0.44467</td>\n",
              "      <td>0.327915</td>\n",
              "      <td>0.321570</td>\n",
              "      <td>0.605077</td>\n",
              "      <td>0.602642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "      <td>...</td>\n",
              "      <td>0.704268</td>\n",
              "      <td>0.178193</td>\n",
              "      <td>0.247408</td>\n",
              "      <td>0.24564</td>\n",
              "      <td>0.22089</td>\n",
              "      <td>0.21230</td>\n",
              "      <td>0.204687</td>\n",
              "      <td>0.202213</td>\n",
              "      <td>0.246011</td>\n",
              "      <td>0.432606</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 130 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a05c5ca5-3f4e-4371-be48-bff5f72088ee')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a05c5ca5-3f4e-4371-be48-bff5f72088ee button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a05c5ca5-3f4e-4371-be48-bff5f72088ee');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-09e89e30-a8d7-45ae-96e9-4af4e003c9b4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-09e89e30-a8d7-45ae-96e9-4af4e003c9b4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-09e89e30-a8d7-45ae-96e9-4af4e003c9b4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "  cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9 cat10  ...     cont5     cont6  \\\n",
              "0    A    B    A    B    A    A    A    A    B     A  ...  0.310061  0.718367   \n",
              "1    A    B    A    A    A    A    A    A    B     B  ...  0.885834  0.438917   \n",
              "2    A    B    A    A    B    A    A    A    B     B  ...  0.397069  0.289648   \n",
              "3    B    B    A    B    A    A    A    A    B     A  ...  0.422268  0.440945   \n",
              "4    A    B    A    B    A    A    A    A    B     B  ...  0.704268  0.178193   \n",
              "\n",
              "      cont7    cont8    cont9   cont10    cont11    cont12    cont13    cont14  \n",
              "0  0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493  0.714843  \n",
              "1  0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431  0.304496  \n",
              "2  0.315545  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709  0.774425  \n",
              "3  0.391128  0.31796  0.32128  0.44467  0.327915  0.321570  0.605077  0.602642  \n",
              "4  0.247408  0.24564  0.22089  0.21230  0.204687  0.202213  0.246011  0.432606  \n",
              "\n",
              "[5 rows x 130 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = pd.read_csv('/content/Data/All Claims.csv')\n",
        "\n",
        "# Display the first few rows of the training data\n",
        "print(\"Training Data:\")\n",
        "# print(train.head())\n",
        "\n",
        "train.drop(columns=['id'],axis=1,inplace=True)\n",
        "\n",
        "X = train.drop(columns=['loss'])\n",
        "y = train['loss']\n",
        "\n",
        "\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-r4zOJIwkmp"
      },
      "outputs": [],
      "source": [
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTMpo3HS4Drb",
        "outputId": "434092d0-8b29-4662-e08a-58769c12a93e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Categorical variables detected. Using 'dice' metric for UMAP.\n",
            "Sampled Data: 45196 Data: 150654\n",
            "Time taken for Dim Red and Grid Search Sampling: 79.40841841697693 seconds\n",
            "Sampled Data: 15066 Data: 150654\n",
            "Time taken for Isolation Forest and KS Sampling: 2.5740365982055664 seconds\n",
            "Sampled Data: 45196 Data: 150654\n",
            "Time taken for Entropy Sampling: 2.318875312805176 seconds\n"
          ]
        }
      ],
      "source": [
        "#### SAMPLING METHODS ####\n",
        "data = X_train1\n",
        "# ================================ #\n",
        "##### DIM REDUCTION AND GRID SEARCH\n",
        "# ================================ #\n",
        "start_time = time.time()\n",
        "stratified_sampler = StratifiedSamplingUMAP(n_neighbors=15, min_dist=0.1, n_components=3, n_grids=10, sample_percentage=0.3)\n",
        "embedding = stratified_sampler.fit_transform(data)\n",
        "sampled_embedding = stratified_sampler.stratified_sampling(embedding)\n",
        "sample_indices = stratified_sampler.get_sample_indices()\n",
        "print(\"Sampled Data:\", len(sample_indices),\"Data:\",len(data))\n",
        "X_train1_1 = data.iloc[sample_indices]\n",
        "y_train1_1 = y_train1.iloc[sample_indices]\n",
        "print(\"Time taken for Dim Red and Grid Search Sampling:\", time.time() - start_time, \"seconds\")\n",
        "\n",
        "# # ================================ #\n",
        "# ##### HDBSCAN - CLUSTERING BASED SAMPLING\n",
        "# # ================================ #\n",
        "\n",
        "# start_time = time.time()\n",
        "# sampler = ClusterSampler(data, sampling_percent=30, min_cluster_size=15, min_samples=3)\n",
        "# sampled_indices = sampler.run()\n",
        "# X_train1_2 = data.loc[sampled_indices]\n",
        "# y_train1_2 = y_train1.iloc[sampled_indices]\n",
        "# print(\"Sampled Data:\", len(sampled_indices),\"Data:\",len(data))\n",
        "# print(\"Time taken for Clustering Based Sampling:\", time.time() - start_time, \"seconds\")\n",
        "\n",
        "# ================================ #\n",
        "##### ISOLATION FOREST AND KS STATISTIC SAMPLER #####\n",
        "# ================================ #\n",
        "\n",
        "start_time = time.time()\n",
        "sampler = AnomalySampler(data)\n",
        "sample_indices = sampler.find_sample_indices()\n",
        "X_train1_3 = data.iloc[sample_indices]\n",
        "y_train1_3 = y_train1.iloc[sample_indices]\n",
        "print(\"Sampled Data:\", len(sample_indices),\"Data:\",len(data))\n",
        "print(\"Time taken for Isolation Forest and KS Sampling:\", time.time() - start_time, \"seconds\")\n",
        "\n",
        "# ================================ #\n",
        "##### ENTROPY SAMPLER #####\n",
        "# ================================ #\n",
        "\n",
        "start_time = time.time()\n",
        "sampler = EntropySampler(data,sampling_percent=30)\n",
        "sample_indices = sampler.run()\n",
        "X_train1_4 = data.iloc[sample_indices]\n",
        "y_train1_4 = y_train1.iloc[sample_indices]\n",
        "print(\"Sampled Data:\", len(sample_indices),\"Data:\",len(data))\n",
        "print(\"Time taken for Entropy Sampling:\", time.time() - start_time, \"seconds\")\n",
        "\n",
        "# ================================ #\n",
        "##### DISTANCE BASED SAMPLER #####\n",
        "# ================================ #\n",
        "\n",
        "start_time = time.time()\n",
        "sampler = DistanceBasedSampler(data, k=5, sampling_percent=30)\n",
        "sampled_indices = sampler.run()\n",
        "X_train1_5 = data.iloc[sampled_indices]\n",
        "y_train1_5 = y_train1.iloc[sampled_indices]\n",
        "print(\"Sampled Data:\", len(sampled_indices),\"Data:\",len(data))\n",
        "print(\"Time taken for Distance Based Sampling:\", time.time() - start_time, \"seconds\")\n",
        "\n",
        "# ================================ #\n",
        "##### RANDOM SAMPLER #####\n",
        "# ================================ #\n",
        "\n",
        "start_time = time.time()\n",
        "sampler = RandomSampler(data=data, sampling_percent=30)\n",
        "sampled_indices = sampler.sample_indices()\n",
        "X_train1_6 = X_train1.iloc[sampled_indices]\n",
        "y_train1_6 = y_train1.iloc[sampled_indices]\n",
        "print(\"Sampled Data:\", len(sampled_indices),\"Data:\",len(data))\n",
        "print(\"Time taken for Random Sampling:\", time.time() - start_time, \"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrPoyf0h4Hrv"
      },
      "outputs": [],
      "source": [
        "# ================================ #\n",
        "##### Trainer for All State Sampling Methods\n",
        "# ================================ #\n",
        "trainer = ModelTrainer(\n",
        "    datasets_X=[\n",
        "        [X_train1_1, X_test1],\n",
        "        # [X_train1_2, X_test1],\n",
        "        [X_train1_3, X_test1],\n",
        "        [X_train1_4, X_test1],\n",
        "        [X_train1_5, X_test1],\n",
        "        [X_train1_6, X_test1]\n",
        "    ],\n",
        "    datasets_y=[\n",
        "        [y_train1_1, y_test1],\n",
        "        # [y_train1_2, y_test1],\n",
        "        [y_train1_3, y_test1],\n",
        "        [y_train1_4, y_test1],\n",
        "        [y_train1_5, y_test1],\n",
        "        [y_train1_6, y_test1]\n",
        "    ],\n",
        "    task_type='classification',\n",
        "    dataset_names=[\n",
        "        'Dim Reduction',\n",
        "        'Clustering',\n",
        "        'Anomaly Detection',\n",
        "        'Entropy',\n",
        "        'Distance Based',\n",
        "        'Random'\n",
        "    ]\n",
        ")\n",
        "\n",
        "results = trainer.train_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJNQQkUc60mx"
      },
      "outputs": [],
      "source": [
        "results.to_csv(\"All Claims.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtjGJOybmDxi"
      },
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYo9W66fmEE0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMJ/kF2D2wQn9FyvM0Vf+gr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}