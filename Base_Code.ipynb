{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMTcO762dyktwXj5yQiAzEo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vibhuverma17/MLBASEDSAMPLING/blob/main/Base_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aad7i3jF5Rcf"
      },
      "outputs": [],
      "source": [
        "!pip install umap-learn hdbscan gower kmodes XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Standard Libraries\n",
        "# ===============================\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import ast\n",
        "\n",
        "# ===============================\n",
        "# Visualization Libraries\n",
        "# ===============================\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "\n",
        "# ===============================\n",
        "# Scikit-learn Components\n",
        "# ===============================\n",
        "# Data Splitting and Preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Models\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, RandomForestRegressor,\n",
        "    GradientBoostingClassifier, GradientBoostingRegressor,\n",
        "    IsolationForest\n",
        ")\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import (\n",
        "    f1_score, roc_auc_score, accuracy_score, recall_score, precision_score,\n",
        "    mean_absolute_error, mean_squared_error, r2_score,\n",
        "    mean_absolute_percentage_error, roc_curve\n",
        ")\n",
        "\n",
        "# Pairwise distances\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "\n",
        "# ===============================\n",
        "# External Libraries\n",
        "# ===============================\n",
        "import xgboost as xgb  # XGBoost library\n",
        "from scipy.stats import ks_2samp, entropy  # Statistical tests\n",
        "from kmodes.kprototypes import KPrototypes  # Clustering\n",
        "import umap  # Dimensionality reduction\n",
        "import hdbscan  # Density-based clustering\n",
        "import gower  # Gower similarity for mixed data types\n",
        "\n",
        "# ===============================\n",
        "# Configure Warnings\n",
        "# ===============================\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "ov6EwiM05WKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir Data"
      ],
      "metadata": {
        "id": "9M0kBWKKsi6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### READING DATA"
      ],
      "metadata": {
        "id": "DuZdfS3kN2OZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/Data/All Claims.csv')\n",
        "\n",
        "# Display the first few rows of the training data\n",
        "print(\"Training Data:\")\n",
        "# print(train.head())\n",
        "\n",
        "train.drop(columns=['id'],axis=1,inplace=True)\n",
        "\n",
        "X = train.drop(columns=['loss'])\n",
        "y = train['loss']\n",
        "\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "# Convert categorical features to category dtype\n",
        "for feature in categorical_features:\n",
        "    X[feature] = X[feature].astype('category')\n",
        "\n",
        "X.head()"
      ],
      "metadata": {
        "id": "5m4zGB5rsjZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "r-r4zOJIwkmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/Data/TUANDROMD.csv')\n",
        "\n",
        "# Display the first few rows of the training data\n",
        "print(\"Training Data:\")\n",
        "\n",
        "train = train[~(train['Label'].isnull())]\n",
        "train.columns = train.columns.str.replace(r'[^a-zA-Z0-9_]', '_', regex=True)\n",
        "\n",
        "X = train.drop(columns=['Label'])\n",
        "y = train['Label']\n",
        "\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "# Convert categorical features to category dtype\n",
        "for feature in categorical_features:\n",
        "    X[feature] = X[feature].astype('category')\n",
        "\n",
        "X.head()"
      ],
      "metadata": {
        "id": "JlHAGsBAGDLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)"
      ],
      "metadata": {
        "id": "XcXpRmC-Gvd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/Data/Andriod Permissions.csv')\n",
        "\n",
        "# Display the first few rows of the training data\n",
        "print(\"Training Data:\")\n",
        "\n",
        "train = train[~(train['Result'].isnull())]\n",
        "train.columns = train.columns.str.replace(r'[^a-zA-Z0-9_]', '_', regex=True)\n",
        "\n",
        "X = train.drop(columns=['Result'])\n",
        "y = train['Result']\n",
        "\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "# Convert categorical features to category dtype\n",
        "for feature in categorical_features:\n",
        "    X[feature] = X[feature].astype('category')\n",
        "\n",
        "X.head()"
      ],
      "metadata": {
        "id": "zdB8hq8MMg5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)"
      ],
      "metadata": {
        "id": "hA6tSbt5Mg1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "train = pd.read_csv('/content/Data/Student Success.csv', delimiter=';')\n",
        "\n",
        "# Clean and preprocess\n",
        "train = train[~train['Target'].isnull()]  # Remove rows with null Target\n",
        "train.columns = train.columns.str.replace(r'[^a-zA-Z0-9_]', '_', regex=True)  # Clean column names\n",
        "train = train[~(train['Target'] == 'Enrolled')]  # Remove 'Enrolled' from Target\n",
        "\n",
        "# Label encode the Target column\n",
        "label_encoder = LabelEncoder()\n",
        "train['Target'] = label_encoder.fit_transform(train['Target'])  # Encode Target as 0 and 1\n",
        "\n",
        "# Split features and target\n",
        "X = train.drop(columns=['Target'])\n",
        "y = train['Target']\n",
        "\n",
        "# Process categorical features\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "for feature in categorical_features:\n",
        "    X[feature] = X[feature].astype('category')\n",
        "\n",
        "X.head()"
      ],
      "metadata": {
        "id": "Ad2marcf9vbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)"
      ],
      "metadata": {
        "id": "1O_CaKiw-yRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/Data/Phishing URL.csv')\n",
        "\n",
        "train = train[[x for x in train.columns if x not in ['FILENAME','URL','Domain','Title']]]\n",
        "\n",
        "# Clean and preprocess\n",
        "train = train[~train['label'].isnull()]  # Remove rows with null Target\n",
        "train.columns = train.columns.str.replace(r'[^a-zA-Z0-9_]', '_', regex=True)  # Clean column names\n",
        "\n",
        "train = train.head(10000)\n",
        "# Split features and target\n",
        "X = train.drop(columns=['label'])\n",
        "y = train['label']\n",
        "\n",
        "# Process categorical features\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "for feature in categorical_features:\n",
        "    X[feature] = X[feature].astype('category')\n",
        "\n",
        "X.head()"
      ],
      "metadata": {
        "id": "CvU4MWgIm325"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train5, X_test5, y_train5, y_test5 = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)"
      ],
      "metadata": {
        "id": "JOBKeomjm357"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/Data/parkinsons_updrs.data')\n",
        "\n",
        "# Display the first few rows of the training data\n",
        "print(\"All Data:\")\n",
        "# print(train.head())\n",
        "\n",
        "train.drop('subject#', axis=1, inplace=True)\n",
        "train.drop('test_time', axis=1, inplace=True)\n",
        "train.drop('total_UPDRS', axis=1, inplace=True)\n",
        "\n",
        "X=train.drop('motor_UPDRS', axis=1)\n",
        "y=train[['motor_UPDRS']]\n",
        "\n",
        "# Replace positive and negative infinity with NaN across the DataFrame\n",
        "X = X.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "X.head()"
      ],
      "metadata": {
        "id": "Aa7Ui_dJ5-I9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train6, X_test6, y_train6, y_test6 = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "NHzs-DD-70Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/Data/creditcard.csv')\n",
        "\n",
        "# Display the first few rows of the training data\n",
        "print(\"Training Data:\")\n",
        "# print(train.head())\n",
        "\n",
        "print(train.shape)\n",
        "\n",
        "train.drop(columns=['Time'],axis=1)\n",
        "# test.drop(columns=['id'],axis=1)\n",
        "\n",
        "# Split the data into features (X) and target (y\n",
        "X = train.drop(columns=['Time','Class'])\n",
        "y = train['Class']\n",
        "\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "# Convert categorical features to category dtype\n",
        "for feature in categorical_features:\n",
        "    X[feature] = X[feature].astype('category')\n",
        "# Split the dataset into training and testing sets\n",
        "X.head()"
      ],
      "metadata": {
        "id": "ZlASYXu4OIBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train7, X_test7, y_train7, y_test7 = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "WEmO1CwBOIaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom transformer to drop highly correlated features\n",
        "class DropHighCorrelation(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, threshold=0.6):\n",
        "        \"\"\"\n",
        "        Custom transformer to drop highly correlated features.\n",
        "\n",
        "        Parameters:\n",
        "        - threshold: Correlation threshold above which features will be dropped.\n",
        "        \"\"\"\n",
        "        self.threshold = threshold\n",
        "        self.to_drop = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Identify features to drop based on the correlation threshold.\n",
        "        \"\"\"\n",
        "        corr_matrix = pd.DataFrame(X).corr().abs()  # Compute the absolute correlation matrix\n",
        "        upper_tri = corr_matrix.where(\n",
        "            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
        "        )  # Extract upper triangle\n",
        "        self.to_drop = [\n",
        "            column for column in upper_tri.columns if any(upper_tri[column] > self.threshold)\n",
        "        ]\n",
        "\n",
        "        print(f\"DropHighCorrelation: {len(self.to_drop)} columns will be dropped due to correlation (threshold={self.threshold}).\")\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Drop the identified features from the dataset.\n",
        "        \"\"\"\n",
        "        return pd.DataFrame(X).drop(columns=self.to_drop, errors='ignore')\n",
        "\n",
        "# Main ModelTrainer class\n",
        "class ModelTrainer:\n",
        "    def __init__(self, datasets_X, datasets_y, task_type, dataset_names=None):\n",
        "        \"\"\"\n",
        "        Initializes the ModelTrainer with datasets, explicitly provided task type, and dataset names.\n",
        "\n",
        "        Parameters:\n",
        "        - datasets_X: List of [X_train, X_test] for different datasets\n",
        "        - datasets_y: List of [y_train, y_test] for different datasets\n",
        "        - task_type: A string explicitly specifying the task type, either 'regression' or 'classification'.\n",
        "        - dataset_names: List of names for the datasets to be used as index in the results DataFrame\n",
        "        \"\"\"\n",
        "        if task_type not in ['classification', 'regression']:\n",
        "            raise ValueError(\"task_type must be either 'classification' or 'regression'\")\n",
        "\n",
        "        self.datasets_X = datasets_X\n",
        "        self.datasets_y = datasets_y\n",
        "        self.task_type = task_type\n",
        "        self.dataset_names = dataset_names\n",
        "\n",
        "    def _select_model(self):\n",
        "        \"\"\"Select model based on task type.\"\"\"\n",
        "        if self.task_type == 'classification':\n",
        "            return {\n",
        "                'RandomForest': RandomForestClassifier(random_state=42),\n",
        "                'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
        "                'LogisticRegression': LogisticRegression(),\n",
        "                'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "            }\n",
        "        elif self.task_type == 'regression':\n",
        "            return {\n",
        "                'RandomForest': RandomForestRegressor(random_state=42),\n",
        "                'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
        "                'LinearRegression': LinearRegression(),\n",
        "                'XGBoost': xgb.XGBRegressor(random_state=42)\n",
        "            }\n",
        "\n",
        "    def _create_pipeline(self, model, X):\n",
        "        \"\"\"Create a preprocessing and modeling pipeline.\"\"\"\n",
        "        # Identify categorical and numerical features\n",
        "        categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "        numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "        # Preprocessing for numerical features: Standard scaling\n",
        "        numerical_transformer = StandardScaler()\n",
        "\n",
        "        # Preprocessing for categorical features: One-hot encoding\n",
        "        categorical_transformer = OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False)\n",
        "\n",
        "        # Combine preprocessors in a column transformer\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numerical_transformer, numerical_features),\n",
        "                ('cat', categorical_transformer, categorical_features)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Define a pipeline with preprocessing, correlation dropping, and the specified model\n",
        "        pipeline = Pipeline(steps=[\n",
        "            ('preprocessor', preprocessor),  # First preprocess\n",
        "            ('drop_high_corr', DropHighCorrelation(threshold=0.6)),  # Then drop highly correlated features\n",
        "            ('model', model)  # Finally, apply the model\n",
        "        ])\n",
        "        return pipeline\n",
        "\n",
        "    def _get_best_cutoff(self, y_true, y_pred_proba):\n",
        "        \"\"\"Use Youden's J statistic to determine the best cutoff point for classification.\"\"\"\n",
        "        fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
        "        youden_index = tpr - fpr\n",
        "        best_cutoff = thresholds[np.argmax(youden_index)]\n",
        "        return best_cutoff\n",
        "\n",
        "    def _train_and_evaluate(self, model, X_train, X_test, y_train, y_test):\n",
        "        \"\"\"Train the model and evaluate it on both the training and test datasets.\"\"\"\n",
        "        start_time = time.time()\n",
        "        model.fit(X_train, y_train)\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        y_pred_train = model.predict(X_train)\n",
        "        y_pred_test = model.predict(X_test)\n",
        "\n",
        "        if self.task_type == 'classification':\n",
        "            y_pred_proba_test = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else y_pred_test\n",
        "            y_pred_proba_train = model.predict_proba(X_train)[:, 1] if hasattr(model, 'predict_proba') else y_pred_train\n",
        "\n",
        "            best_cutoff = self._get_best_cutoff(y_test, y_pred_proba_test)\n",
        "            y_pred_class_test = (y_pred_proba_test >= best_cutoff).astype(int)\n",
        "            y_pred_class_train = (y_pred_proba_train >= best_cutoff).astype(int)\n",
        "\n",
        "            metrics = {\n",
        "                'Train F1': f1_score(y_train, y_pred_class_train),\n",
        "                'Test F1': f1_score(y_test, y_pred_class_test),\n",
        "                'Train AUC': roc_auc_score(y_train, y_pred_proba_train),\n",
        "                'Test AUC': roc_auc_score(y_test, y_pred_proba_test),\n",
        "                'Train Accuracy': accuracy_score(y_train, y_pred_class_train),\n",
        "                'Test Accuracy': accuracy_score(y_test, y_pred_class_test),\n",
        "                'Train Recall': recall_score(y_train, y_pred_class_train),\n",
        "                'Test Recall': recall_score(y_test, y_pred_class_test),\n",
        "                'Train Precision': precision_score(y_train, y_pred_class_train),\n",
        "                'Test Precision': precision_score(y_test, y_pred_class_test),\n",
        "                'Best Cutoff': best_cutoff,\n",
        "                'Training Time (seconds)': training_time\n",
        "            }\n",
        "        else:\n",
        "            metrics = {\n",
        "                'Train MSE': mean_squared_error(y_train, y_pred_train),\n",
        "                'Test MSE': mean_squared_error(y_test, y_pred_test),\n",
        "                'Train MAPE': mean_absolute_percentage_error(y_train, y_pred_train),\n",
        "                'Test MAPE': mean_absolute_percentage_error(y_test, y_pred_test),\n",
        "                'Train R2': r2_score(y_train, y_pred_train),\n",
        "                'Test R2': r2_score(y_test, y_pred_test),\n",
        "                'Training Time (seconds)': training_time\n",
        "            }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def train_models(self):\n",
        "        \"\"\"Train and evaluate models on multiple datasets and return a DataFrame of results.\"\"\"\n",
        "        models = self._select_model()\n",
        "        results = []\n",
        "\n",
        "        for idx, (X_data, y_data) in enumerate(zip(self.datasets_X, self.datasets_y)):\n",
        "            X_train, X_test = X_data\n",
        "            y_train, y_test = y_data\n",
        "\n",
        "            for model_name, model in models.items():\n",
        "                pipeline = self._create_pipeline(model, X_train)\n",
        "                metrics = self._train_and_evaluate(pipeline, X_train, X_test, y_train, y_test)\n",
        "                metrics['Dataset'] = self.dataset_names[idx] if self.dataset_names else f'Dataset {idx+1}'\n",
        "                metrics['Model'] = model_name\n",
        "                results.append(metrics)\n",
        "\n",
        "        return pd.DataFrame(results).set_index('Dataset')"
      ],
      "metadata": {
        "id": "LZegGUMVwmlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume you have the datasets as before\n",
        "trainer = ModelTrainer(\n",
        "    datasets_X=[[X_train1, X_test1],[X_train6, X_test6],[X_train7, X_test7]],\n",
        "    datasets_y=[[y_train1, y_test1],[y_train6, y_test6],[y_train7, y_test7]],\n",
        "    task_type='regression',\n",
        "    dataset_names=['All Claims','Parkinsons','Credit Card Fraud']\n",
        ")\n",
        "\n",
        "results = trainer.train_models()"
      ],
      "metadata": {
        "id": "rcrmOYAJ_JY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.to_csv('regression.csv',index=False)"
      ],
      "metadata": {
        "id": "AAgGLNzUA01Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "EAtDJYJqQfhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume you have the datasets as before\n",
        "trainer = ModelTrainer(\n",
        "    datasets_X=[[X_train2, X_test2],[X_train3, X_test3],[X_train4, X_test4],[X_train5, X_test5]],\n",
        "    datasets_y=[[y_train2, y_test2],[y_train3, y_test3],[y_train4, y_test4],[y_train5, y_test5]],\n",
        "    task_type='classification',\n",
        "    dataset_names=['Tuandromd','Andriod Permissions','Student Success','Phishing URL']\n",
        ")\n",
        "\n",
        "results = trainer.train_models()"
      ],
      "metadata": {
        "id": "JvLU-psNEFuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.to_csv('classification.csv',index=False)"
      ],
      "metadata": {
        "id": "rlKOi6LWaiJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "Dv8sIvFAQhmN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}